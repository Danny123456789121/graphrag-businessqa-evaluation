{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and init models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from datasets import Dataset\n",
    "from rag_utils import setup_rag_embeddings, build_rag_chain\n",
    "from ragas import SingleTurnSample, EvaluationDataset\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from ragas.metrics import Faithfulness, FactualCorrectness\n",
    "from ragas import evaluate\n",
    "from gen_ai_hub.proxy.langchain.openai import ChatOpenAI\n",
    "from gen_ai_hub.proxy.core.proxy_clients import get_proxy_client\n",
    "from gen_ai_hub.proxy.langchain.openai import OpenAIEmbeddings\n",
    "\n",
    "import asyncio\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "\n",
    "load_dotenv(override=True)\n",
    "DATA_DIR = \"reports/\"\n",
    "\n",
    "proxy_client = get_proxy_client('gen-ai-hub')\n",
    "llm_sap = ChatOpenAI(proxy_model_name=\"gpt-4o\", proxy_client=proxy_client)\n",
    "embeddings_model = OpenAIEmbeddings(proxy_model_name='text-embedding-ada-002', proxy_client=proxy_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "setup_rag_embeddings() got an unexpected keyword argument 'faiss_path'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#setup embeddings and chain\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m retriever \u001b[38;5;241m=\u001b[39m \u001b[43msetup_rag_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDATA_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfaiss_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfaiss_1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membeddings_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m rag_chain \u001b[38;5;241m=\u001b[39m build_rag_chain(retriever\u001b[38;5;241m=\u001b[39mretriever, llm\u001b[38;5;241m=\u001b[39mllm)\n",
      "\u001b[1;31mTypeError\u001b[0m: setup_rag_embeddings() got an unexpected keyword argument 'faiss_path'"
     ]
    }
   ],
   "source": [
    "#setup embeddings and chain\n",
    "retriever = setup_rag_embeddings(data_dir=DATA_DIR, faiss_path=\"faiss_1\", embeddings_model=embeddings_model)\n",
    "rag_chain = build_rag_chain(retriever=retriever, llm=llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map generated questions and ground_truths into evaluation dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('synthetic_data_big_context.json', 'r') as f:\n",
    "    synthetic_data = json.load(f)\n",
    "\n",
    "if isinstance(synthetic_data, dict) and 'responses' in synthetic_data:\n",
    "    synthetic_data = synthetic_data['responses']\n",
    "\n",
    "queries = [item.get('question', \"\") for item in synthetic_data]\n",
    "ground_truths = [item.get('ground_truth', \"\") for item in synthetic_data]\n",
    "contexts = [item.get('context', \"\") for item in synthetic_data]\n",
    "\n",
    "answers = []\n",
    "retrieved_contexts = []\n",
    "\n",
    "for query in queries:\n",
    "    answer = rag_chain.invoke(query)\n",
    "    answers.append(answer)\n",
    "    print(\"Query: \",query)\n",
    "    print(\"Anwer: \",answer)\n",
    "    retrieved_context = [doc.page_content for doc in retriever.invoke(query)]\n",
    "    retrieved_contexts.append(retrieved_context)\n",
    "    print(\"Retrieved context:\",retrieved_context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# populate eval dataset\n",
    "\n",
    "evaluation_samples = []\n",
    "\n",
    "for query, answer, retrieved_context, ground_truth in zip(queries, answers, retrieved_contexts, ground_truths):\n",
    "    sample = SingleTurnSample(\n",
    "        user_input=query,\n",
    "        response=answer,\n",
    "        reference=ground_truth,\n",
    "        retrieved_contexts=retrieved_context\n",
    "    )\n",
    "    evaluation_samples.append(sample)\n",
    "\n",
    "evaluation_dataset = EvaluationDataset(samples=evaluation_samples)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(evaluation_dataset.samples)\n",
    "print(type(evaluation_dataset.samples))\n",
    "print(type(evaluation_dataset.samples[0]))\n",
    "print(dir(evaluation_dataset.samples[0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "evaluator_llm = LangchainLLMWrapper(llm)\n",
    "evaluator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings())\n",
    "\n",
    "\n",
    "metrics = [\n",
    "    FactualCorrectness(llm=evaluator_llm, mode=\"precision\", name=\"FactualCorrectness_Precision\"),\n",
    "    FactualCorrectness(llm=evaluator_llm, mode=\"recall\", name=\"FactualCorrectness_Recall\"),\n",
    "    FactualCorrectness(llm=evaluator_llm, mode=\"f1\", name=\"FactualCorrectness_F1\"),\n",
    "    Faithfulness(llm=evaluator_llm), \n",
    " \n",
    "]\n",
    "results = evaluate(dataset=evaluation_dataset, metrics=metrics)\n",
    "df = results.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 250) # default is 50 / None for unlimited\n",
    "\n",
    "df\n",
    "\n",
    "# context recall = measures how many of the relevant documents (or pieces of information) were successfully retrieved\n",
    "# factual correctness (precision) = proportion of correct claims made that also found in the reference / how many are correct \n",
    "# factual correctness (recall) = proportion of facts in the reference that are also present in the response / how many are found\n",
    "# faithfulness = are the claims made in the response supported by the retrieved context\n",
    "# semantic similarity = how similar the response is to the ground truth\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "source": [
    "# GraphRAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graphrag initial setup\n",
    "!mkdir -p ./graphrag\n",
    "!python -m graphrag init --root ./graphrag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indexing, only run once unless you want to update the index\n",
    "# !graphrag index --root ./graphrag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Search\n",
    "\n",
    "follows the implementations guide by the docs https://microsoft.github.io/graphrag/examples_notebooks/global_search/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.graphrag_utils import setup_graphrag\n",
    "import os\n",
    "import json\n",
    "import asyncio\n",
    "\n",
    "api_key = os.environ[\"GRAPHRAG_API_KEY\"]\n",
    "community_level = 2\n",
    "model_name = \"gpt-4o\"\n",
    "\n",
    "with open('synthetic_data_big_context.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "    \n",
    "queries = [response['question'] for response in data['responses']]\n",
    "\n",
    "search_engine_global = setup_graphrag(api_key, model_name, community_level)\n",
    "\n",
    "async def perform_global_search(query):\n",
    "    print(f\"Performing search with query: {query}\")\n",
    "    result = await search_engine_global.asearch(query)\n",
    "    print(f\"Result for query: {query} is: {result.response}\")\n",
    "    return result.response\n",
    "\n",
    "tasks = [perform_global_search(query) for query in queries]\n",
    "results = await asyncio.gather(*tasks)\n",
    "\n",
    "evaluation_samples = []\n",
    "for query, result in zip(queries, results):\n",
    "    sample = SingleTurnSample(\n",
    "        user_input=query,\n",
    "        response=result,\n",
    "    )\n",
    "    evaluation_samples.append(sample)\n",
    "query = \"What is McKinsey ?\"\n",
    "result = await search_engine_global.asearch(query)\n",
    "print(result)\n",
    "\n",
    "evaluation_dataset = EvaluationDataset(samples=evaluation_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_llm = LangchainLLMWrapper(llm)\n",
    "evaluator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings())\n",
    "\n",
    "metrics = [\n",
    "    FactualCorrectness(llm=evaluator_llm, mode=\"precision\", name=\"FactualCorrectness_Precision\"),\n",
    "    FactualCorrectness(llm=evaluator_llm, mode=\"recall\", name=\"FactualCorrectness_Recall\"),\n",
    "    Faithfulness(llm=evaluator_llm), \n",
    "]\n",
    "results = evaluate(dataset=evaluation_dataset, metrics=metrics)\n",
    "df = results.to_pandas()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.set_option('display.max_colwidth', 300) # default is 50 / None for unlimited\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
