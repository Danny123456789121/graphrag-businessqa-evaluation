{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile requirements.txt\n",
    "langchain\n",
    "langchain-openai\n",
    "llama-parse\n",
    "python-dotenv\n",
    "pydantic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries and Load Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import nest_asyncio\n",
    "\n",
    "\n",
    "nest_asyncio.apply()\n",
    "load_dotenv(override=True)\n",
    "\n",
    "data_dir = \"data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse PDF Documents to Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting the-economic-potential-of-generative-ai-the-next-productivity-frontier.pdf to markdown\n",
      "Started parsing the file under job_id 1653af61-9394-4c89-bc69-6aa24d61af1d\n",
      "..Saving markdown to data/the-economic-potential-of-generative-ai-the-next-productivity-frontier.md\n",
      "Successfully converted the-economic-potential-of-generative-ai-the-next-productivity-frontier.pdf\n"
     ]
    }
   ],
   "source": [
    "from llama_parse import LlamaParse\n",
    "\n",
    "for file in os.listdir(data_dir):\n",
    "    if file.endswith(\".pdf\"):\n",
    "        try:\n",
    "            print(f\"Converting {file} to markdown\")\n",
    "            md_text = LlamaParse(result_type=\"markdown\", verbose=True, language=\"en\").load_data(data_dir + file)\n",
    "            combined_md_text = \"\\n\\n\".join([doc.text for doc in md_text])\n",
    "            md_file_path = data_dir + file.replace(\".pdf\", \".md\")\n",
    "            print(f\"Saving markdown to {md_file_path}\")\n",
    "            with open(md_file_path, \"w\") as f:\n",
    "                f.write(combined_md_text)\n",
    "            print(f\"Successfully converted {file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error converting {file}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Dataset from Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Define data models for responses\n",
    "class Response(BaseModel):\n",
    "    question: str = Field(description=\"Question\")\n",
    "    ground_truth: str = Field(description=\"Ground Truth\")\n",
    "    context: str = Field(description=\"Context\")\n",
    "    documents: str = Field(description=\"Name of the documents used\")\n",
    "\n",
    "class Responses(BaseModel):\n",
    "    responses: list[Response] = Field(description=\"List of responses\")\n",
    "\n",
    "# Combine content from all markdown files in the directory\n",
    "documents = []\n",
    "for file in os.listdir(data_dir):\n",
    "    if file.endswith(\".md\"):\n",
    "        file_path = os.path.join(data_dir, file)\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read()\n",
    "            documents.append(Document(page_content=content, metadata={\"source\": file}))\n",
    "combined_documents_content = \"\\n\\n\".join([doc.page_content for doc in documents])\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=Responses)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"documents\"],\n",
    "    partial_variables={\"format_output\": parser.get_format_instructions()},\n",
    "    template=\"\"\"\\\n",
    "Generate broad questions and their answers (ground truth) along with the context relevant to get that answer. \n",
    "The response should be in JSON format. Don't hallucinate or make up any information. \n",
    "\n",
    "The output MUST strictly adhere to the following JSON format, and NO other text MUST be included:    \n",
    "{format_output}\n",
    "\n",
    "Use only the following documents: \n",
    "<documents>\n",
    "{documents}\n",
    "</documents>\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "model = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0, verbose=True)\n",
    "\n",
    "chain = prompt | model | parser\n",
    "\n",
    "response = chain.invoke({\"documents\": combined_documents_content})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save LLM response to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "with open(\"./synthetic_data_big_context.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(response.dict(), f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
