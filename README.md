# Graphrag Business QA Evaluation

This repository provides the code and resources for evaluating GraphRAG and a baseline RAG technique in the context of complex business question answering (QA). 
The project encompasses two methods for constructing an evaluation dataset along with its associated knowledge base. 
Additionally, it includes two benchmark notebooks that assess the performance of each RAG technique on the respective datasets using RAGAS metrics such as factual correctness, cost, and latency.

# Setup

1. Clone this repository
2. Install Python (3.10-1.12 if using SAP GenAIHub)
3. create & activate virtual env 
4. pip install -r requirements.txt
5. create dotenv containing API Keys (Google Gemini, OpenAI)
6. follow each notebook for either dataset creation or benchmarking

## License
This project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.
