Received: 5 June 2023 &nbsp; &nbsp; &nbsp; Revised: 8 June 2023 &nbsp; &nbsp; &nbsp; Accepted: 9 June 2023

DOI: 10.1111/1748-8583.12524

# INVITED REVIEW

# Human resource management in the age of generative artificial intelligence: Perspectives and research directions on ChatGPT

Pawan Budhwar 1 &nbsp; &nbsp; | &nbsp; &nbsp; Soumyadeb Chowdhury 2 &nbsp; &nbsp; | &nbsp; &nbsp; Geoffrey Wood 3,4,5,6 &nbsp; &nbsp; | &nbsp; &nbsp; Herman Aguinis 7 &nbsp; &nbsp; | &nbsp; &nbsp; Greg J. Bamber 8,9,10 &nbsp; &nbsp; | &nbsp; &nbsp; Jose R. Beltran 9 &nbsp; &nbsp; | &nbsp; &nbsp; Paul Boselie 11 &nbsp; &nbsp; | &nbsp; &nbsp; Fang Lee Cooke 12 &nbsp; &nbsp; | &nbsp; &nbsp; Stephanie Decker 13 &nbsp; &nbsp; | &nbsp; &nbsp; Angelo DeNisi 14 &nbsp; &nbsp; | &nbsp; &nbsp; Prasanta Kumar Dey 15 &nbsp; &nbsp; | &nbsp; &nbsp; David Guest 16 &nbsp; &nbsp; | &nbsp; &nbsp; Andrew J. Knoblich 17 &nbsp; &nbsp; | &nbsp; &nbsp; Ashish Malik 18 &nbsp; &nbsp; | &nbsp; &nbsp; Jaap Paauwe 10 &nbsp; &nbsp; | &nbsp; &nbsp; Savvas Papagiannidis 19 &nbsp; &nbsp; | &nbsp; &nbsp; Charmi Patel 20 &nbsp; &nbsp; | &nbsp; &nbsp; Vijay Pereira 21 &nbsp; &nbsp; | &nbsp; &nbsp; Shuang Ren 22 &nbsp; &nbsp; | &nbsp; &nbsp; Steven Rogelberg 17 &nbsp; &nbsp; | &nbsp; &nbsp; Mark N. K. Saunders 23 &nbsp; &nbsp; | &nbsp; &nbsp; Rosalie L. Tung 24 &nbsp; &nbsp; | &nbsp; &nbsp; Arup Varma 25

# Correspondence

Soumyadeb Chowdhury.
Email: s.chowdhury@tbs-education.fr

# Abstract

ChatGPT and its variants that use generative artificial intelligence (AI) models have rapidly become a focal point in academic and media discussions about their potential benefits and drawbacks across various sectors of the economy, democracy, society, and environment. It remains unclear whether these technologies result in job displacement or creation, or if they merely shift human labour by generating new, potentially trivial or practically irrelevant, information and decisions. According to the CEO

# Abbreviations

AI, artificial intelligence; BERT, bidirectional encoder representations from transformers; EDI, equity diversity inclusion; ER, employee relations; GPT, generative pre-trained transformer; HRM, human resource management; IHRM, international human resource management; IoT, internet of things; IP, intellectual property; IT, information technology; KBV, knowledge based view; LLM, large language model; ML, machine learning; RBV, resource based view; RLHF, reinforcement learning with human feedback; MOOCs, massive open online courses; SaaS, software-as-a-service; SHRM, strategic human resource management.

The contributors and editors are listed in alphabetical order of their surnames. Each perspective is assigned to a section based on its relevance. The sections are organised to make the flow of the narrative logical, easy to navigate, insightful and compelling.

This is an open access article under the terms of the Creative Commons Attribution-NonCommercial-NoDerivs License, which permits use and distribution in any medium, provided the original work is properly cited, the use is non-commercial and no modifications or adaptations are made.

© 2023 The Authors. Human Resource Management Journal published by John Wiley & Sons Ltd.

Hum Resour Manag J. 2023;1–54. wileyonlinelibrary.com/journal/hrmj

# BUDHWAR et al.

of ChatGPT, the potential impact of this new family of AI technology could be as big as “the printing press”, with significant implications for employment, stakeholder relationships, business models, and academic research, and its full consequences are largely undiscovered and uncertain. The introduction of more advanced and potent generative AI tools in the AI market, following the launch of ChatGPT, has ramped up the “AI arms race”, creating continuing uncertainty for workers, expanding their business applications, while heightening risks related to well-being, bias, misinformation, context insensitivity, privacy issues, ethical dilemmas, and security. Given these developments, this perspectives editorial offers a collection of perspectives and research pathways to extend HRM scholarship in the realm of generative AI. In doing so, the discussion synthesizes the literature on AI and generative AI, connecting it to various aspects of HRM processes, practices, relationships, and outcomes, thereby contributing to shaping the future of HRM research.

# K E Y W O R D S

artificial intelligence, ChatGPT, CSR, ethics, generative AI, HRM, human resource strategy, international human resource management, productivity, sustainability

# Key points

# What is currently known?

- The rapid evolution of artificial intelligence models has swiftly prompted much academic and media discourse regarding their potential for disruption as well as their transformative power impacting multiple facets of the economy, society, and environment.
- Software tools like ChatGPT and other comparable ones utilizing generative AI models can produce incredibly human-like responses to queries, yet, they can also be profoundly erroneous, raising significant ethical and moral issues, and their adoption by HRM practitioners.

# What this perspectives editorial adds?

- Provides a comprehensive summary of the advancements, constraints, and commercial applications of generative AI.
- Offers 11 perspectives that advance scholarship in HRM and present a collection of unexplored research opportunities for HRM scholars.

# The implications for practitioners

- Comprehending the possible strengths and weaknesses of implementing immersive technologies like ChatGPT and its variants in HRM strategy, practices, procedures, platforms, and productivity will aid organisations' leaders in critically evaluating its relevance, feasibility to implement, usefulness and potential impact to achieve organisationally valued outcomes.
- The lack of regulations heightens the risks and ethical dilemmas associated with the usage of generative AI models, which presents significant threats for organisations, scholarly research, and society at large.

This HRMJ perspectives editorial brings together a collection of viewpoints on how we can advance scholarship in human resource management (HRM) considering the rapid developments in generative artificial intelligence (AI) and their emerging implications for work processes in general and HRM in particular. In doing so, the primary purpose is twofold: first, to provide an overview regarding the developments in the field, and two, to develop a portfolio of research opportunities that will help HRM scholars to engage in impactful research advancing our understanding about generative AI, by theorizing and providing empirical evidence, to push the existing scientific boundaries. To get a comprehensive overview of the continuously evolving scene, we invited leading scholars to provide perspectives into key themes core to HRM. The perspectives underscore the need for HR professionals to understand and adapt to the changing AI landscape, particularly the transformative and disruptive potentials of generative AI on HRM planning, practices, processes, platforms, and productivity.

# 1 | THE RISE OF GENERATIVE AI – UNRAVELLING THE KNOWNS AND UNKNOWNS

Pawan Budhwar1, Soumyadeb Chowdhury2, and Geoffrey Wood3,4,5,6

Artificial Intelligence (AI) tools and digital platforms, or at least tools and platforms that claim “intelligent status” have become an indispensable part for business organisations and society over the past decade. This stems from AI algorithms' ability to automate business processes, extract knowledge from big data, provide predictions and recommendations, and superior analytical and computational capabilities compared to human beings (von Krogh, Roberson and Gruber, 2023). The various forms of AI such as robotic process automation (e.g., cobots in warehouses), computer vision techniques, speech recognition, machine and deep learning algorithms, and natural language processing have created a plethora of opportunities and unique capabilities for organisations to redesign business processes and functions, innovate business models and offerings in consumer space (like data-driven agile and objective decision-making, project management and strategizing critical productivity indicators) (Kiron, 2022; Schrage et al., 2023). At the same time, as with many recent technological advances, it is difficult to disentangle hype from substance, and AI has proven more successful in some areas (e.g., generating superficially credible written text) than others (e.g., robots with genuine dexterity). It remains uncertain if generative AI deskills or destroys jobs, and/or creates new ones. It is paradoxical insofar as it takes work out of human hands and creates more work for humans by generating new information and decisions to be made in areas that may be of major importance or trivial.

In the past 5 years, human resource management (HRM) scholarship on AI has increased (Budhwar et al., 2022; Chowdhury et al., 2023; Edwards et al., 2022; Malik, Budhwar, & Kazmi, 2022). For instance, recent studies have emphasised the benefits of employing AI-based machine learning (ML) tools to promote diversity (Daugherty

BUDHWAR et al.

et al., 2018), employee recruitment (Pan et al., 2022), factors influencing AI adoption among HRM managers (Suseno et al., 2022), increasing role of bots to enhance employee workplace experience (Malik, Budhwar, Patel, & Srikanth, 2022), the impact of AI on the job roles, responsibilities, tasks and meaningfulness of work (Wilson et al., 2017), the importance of embedding transparency within the AI algorithms (Chowdhury et al., 2022) and tensions that exist in creating algorithmic inclusion (Kelan, 2023). In this context, recent reviews have outlined the role of AI to facilitate HR analytics (Margherita, 2022), and its potential impact on HRM processes and practices (Pereira et al., 2023). However, it has also been linked to the deskilling of professional work (Xue et al., 2022), and potentially, may induce unethical management decision making.

However, AI systems and types have also swiftly evolved creating new opportunities and challenges for both academic and business practitioners. Such is the case of Generative AI, which has become popular since a conventional bot called ChatGPT (Generative Pre-trained Transformer) was first released for public use in November 2022, followed by a superior version in March 2023 (ChatGPT-4) (OpenAI Blog, 2022). ChatGPT has gained significant popularity since the launch for its ability to generate compelling human-like answers to almost any question asked. The ChatGPT-human interaction is realistic and conversational in the sense that the bot can answer follow-up questions, admit its mistakes, and eject inappropriate requests. ChatGPT represents a significant departure from conventional AI algorithms based on ML which can identify patterns in large datasets and make predictions. We can observe this predictive ability in search engines like Google, which offer autocomplete recommendations to enhance quality of search results. However, ChatGPT goes beyond mere prediction. It utilizes generative AI language models, enabling it to generate entirely new content based solely on the question prompts provided by a user. This textual content can take various forms, such as news articles, poetry, movie scripts, business plans, software codes, research manuscripts, and marketing campaigns. The quality of its output relies on the quality of the input it receives, considering both the training data it has been exposed to and the prompts provided by users to describe the task they want it to accomplish.

The language model (“brain” behind the ChatGPT) uses generative AI. Generative AI refers to integration of ML models to generate new content, including text, audio, video, images, software code and simulations, based on large datasets used to train the model. However, the contextual relevance of new content generated will depend on the quality, timeliness, and relevance of the training dataset (Boston Consulting Group Generative AI, 2023; McKinsey & Featured Insights, 2023). For example, GPT4 is trained using around 45 terabytes of data gathered from all over the web only until 2021 (Open AI GPT 4 Technical Report, 2023). Therefore, the responses are unlikely to provide recent and up-to-date information.

Let us uncover the history behind ChatGPT to better understand its evolution (Figure 1). Large Language Models (LLMs) are trained using vast amount of textual data and they infer relationships between words within the text to predict the next word in a sequence of words to form a sentence. However, initial LLMs processed each word individually and sequentially in a block of text, limiting the context, meaningfulness, and efficiency of the outputs. This led to the development of transformers in 2017, which can process all words in a dataset simultaneously, giving varying weights to different parts of the text, making the outputs more meaningful and accurate (Uszkoreit, 2017). The feature enabled processing significantly large datasets efficiently and led to the development of GPT models.

OpenAI launched GPT-1 in 2018, followed by GPT-2 in 2019, GPT-3 in 2020, Instruct-GPT and ChatGPT in 2022, followed by GPT-4 in March 2023 (see Figure 1). Since its inception, GPT models have been trained using huge datasets (around 575 GB training dataset in case of GPT-3) and significant number of parameters (175 billion in case of GPT-3) to be able to answer question prompts (OpenAI Research Blog LM, 2023). However, GPT-3 would produce incorrect outputs having false information (referred to as hallucinations), which may include harmful and offensive content as there were no safety parameters and may also produce irrelevant results. This led to integrating supervised and reinforcement ML techniques to incorporate human feedback in the training process which will produce outputs aligned to the users' intent and context of the prompt (Ouyang et al., 2023).

# BUDHWAR et al.

# Superior

|1|Similar models launched since GPT-4|
|---|---|
| |Meta ImageBind multisensory AI model, May 2023|
| |Google Project Magi, May 2023|
|7|Google Bard, March 2023|
| |DALL-E2, superior version of DALL-E launched in 2022|
| |Meta LLaMa, February 2023|
| |Generating digital images from natural language descriptions:|
| |GPT-4 factual, generate longer text outputs than around 100 trillion parameters (rumored).|
| |DALL-E, 2021, using GPT-3|
|5|AnstructGPT, ChatGPT, and extract information from images|
|7|GPT-3 generate sensible and useful text fit to the context and conversational feature, using Reinforcement Learning from Human Feedback technique|
|2020|GPT-2 175 billion parameters, generating human-like text, answering questions, writing code, sentences were correct but not fit to the context|
|3|1.5 billion parameters trained on more than 10 times the data used for GPT-1|
|L|GPT-1 2018 117 million parameters trained on Common Crawl; web pages with billions of words and Book Corpus, collection of over 11,000 books on a variety of genres:|
| |Transformer Architecture launched by Google in 2017|

# Inferior

2017

May 2023

# Evolution of GPT.

F I G U R E 1

# 6

# BUDHWAR et al.

Therefore, GPT-3 model was fine-tuned by hiring human contractors to create a labelled training dataset, that is, for each input (collected from past GPT-2 user prompts), contractors will provide an appropriate response, for the model to learn from. This does not, of course, preclude the possibility of biases, or incorrect choices, by the said contractors. This model became GPT-3.5. It was enhanced to better handle diverse prompts using reinforcement learning with human feedback (RLHF) technique (Ouyang et al., 2023). This involved creating a reward model with comparison data, where contractors ranked multiple responses generated by the model for a given prompt, so the AI could learn which response was the most relevant (Schulman et al., 2017). In essence, this allowed humans (or, rather, a few individuals) to guide the model's learning process and refine its ability to produce high-quality responses.

The GPT-4 model was created with the aim of improving alignment between the model's output and user intentions (Open AI GPT 4 Research Blog, 2023). This involves enhancing the accuracy of the model's output and reducing the occurrence of offensive language, but this has done little to resolve AI's signature bland writing style. Compared to its predecessor GPT-3.5, GPT-4 has shown better performance in terms of factual correctness and lower error rates; this does not mean that it could not generate falsities on demand. It also provides greater steerability, allowing users to adjust the model's tone and style to suit their preferences. Additionally, GPT-4 ostensibly incorporates better guardrails to prevent inappropriate responses, demonstrating a greater adherence to ethical considerations. However, these measures are limited to the rules and community standards set by the developers. There are several ways to jailbreak GPT-4, which is a method to use specific prompts bypassing the guardrails and unlocking restricted features that may make GPT-4 spread misinformation and showcase unethical behaviour (Loynds, 2023). Unlike its predecessors, which are limited to processing text, GPT-4 can process both text and images. This allows GPT-4 to analyse the content of an image and connect it with a written question, thus providing a more comprehensive understanding of the context. However, it should be noted that GPT-4 is not designed to generate images, but only to analyse them. Nonetheless, AI's capacity to generate pastiches from styles of art is well known; the same goes for its capacity to generate academic papers.

The authenticity of responses generated by generative AI such as GPT-4 is limited because these algorithms are opaque in the sense that they do not explain how the responses were generated and the source of the training data is also unknown. Moreover, the algorithms' capability to generate false information (somewhat kindly called “hallucinations”), has raised significant concerns on the trustworthiness of the outputs, which may pose serious ethical and reputational risks for organisations planning to use the AI outputs. For example, Google shares lost $100 billion due to an inaccurate response from Bard (Google AI chatbot) on Twitter during a demonstration in February 2023 (Thorbecke, 2023). It is unclear if a deceit and the capacity to generate false information is built into some of the more prominent AI systems or is an autonomous and conscious choice of the latter; each would raise serious ethical concerns.

In the context of business applications of GPT-4, the language learning application Duolingo is utilizing a GPT-4 model to improve personalized learning, Be My Eyes, provider of assistive technology is using it to create a solution for individuals with visual impairments to make them aware of their surroundings (Derico & Kleinman, 2023). Microsoft Bing, 2023 is using GPT-4 to improve search engine user experience, and Stripe, a financial platform, is employing it to safeguard chatrooms from spams and fraudsters (Stripe Newsroom, 2023). Morgan Stanley is using GPT-4 for streamlining internal technical support processes (Davenport, 2023), and Salesforce has implemented it to provide personalised recommendations to help users with their queries (Yakar, 2023). Most notably, government of Iceland has partnered with OpenAI to use GPT-4 to preserve the Icelandic language (OpenAI Customer Stories, 2023). Microsoft Office 365 is integrating GPT-4 to create an entire PowerPoint presentation with a single text prompt or summarize long documents in Word (Microsoft Blog, 2023). The initial business applications of generative AI demonstrate its capabilities, but the ultimate acceptance and adoption of this technology will depend on the resolution of significant ethical and security concerns.

# BUDHWAR et al.

The emergence of ChatGPT in the public domain has also sparked a surge of interest and competition in the AI chatbot market. Microsoft's announcement of a $10 billion investment in OpenAI has further amplified this trend, prompting other technology providers to enter the race. Google has recently launched an experimental service called “Bard”, and ‘Project Magi’ (to revolutionise web search experience) (Data Scientist, 2023), while Meta has released LLaMA, a language model with 65 billion parameters (Meta AI Research, 2023). Baidu has also entered with its ChatGPT-style service called “Wenxin Yiyan” in Chinese or “Ernie Bot” in English. In addition, Character.ai, an AI chatbot is capable of impersonating famous people or fictional characters. Naver, a South Korean search engine firm, has announced plans to launch “SearchGPT” in the first half of 2023. Meanwhile, Yandex, a Russian technology company, has revealed plans to release “YaLM 2.0” in Russian by the end of 2023. Therefore, we anticipate the technology may become more powerful and sophisticated as new players continue to enter the generative AI market, further extending its business applications and potential business risks. However, as any reader who has been unfortunate enough to have engaged with an automated web-based customer response chat system will be aware, such systems are also getting ever better at generating superficially credible waffle and flannel, and batting away unwanted queries, than genuinely working with people to solve problems.

Contrary to the hype build around generative AI systems like GPT-4, organisations have an enormous task to train and operationalise these systems within their existing business processes which will have several implications for HRM. The training process is labour intensive, to contextualise the outputs aligned to the needs of the firms. It also raises issues of the type of training; there is a difference between technical mastery of an AI system and functional knowledge of the area of management or operations it is meant to facilitate. Additionally, Dr Geoffrey Hinton (the so-called “Godfather of AI”) - who recently quit Google - has expressed his apprehensions about the potential risks associated with generative AI, including spreading misinformation, manipulating societal structures, disrupting the job markets, cyber security threats, and exploitation by malicious entities in unforeseen ways which could potentially lead to geo-political tensions, inflicting severe harm upon humanity (Guardian News, 2023). For instance, in the initial version launched in November 2022, which raise concerns about bias, real-world harm, how generative AI models are trained, and uncertainty surrounding the behaviour of ChatGPT (Bidle, 2022). Knowledge as to the emerging risks and threats posed by intelligent technologies, and indeed, regulation, has lagged far behind AI's advances. For instance, the US senate committee following the testimony of CEO of OpenAI has called for a new body to regulate the AI industry, however, considering the rapid pace of technological advancements, lawmakers also questioned whether such a regulatory body would be able to stay abreast of the developments (Clayton, 2023).

AI itself may facilitate the research process but opens real risks given its predilection for concocting information when short of material or when prompted to do so. It also opens the opportunity for automated evaluation of research (Gendron et al., 2022). This automated evaluation can be beneficial in verifying the representation of multiple authentic voices in interview transcripts, ensuring the credibility of results presented in tables, and even confirming the authorship of a paper to prevent instances of outsourcing to AI. However, it also may curtail the chances for human judgement based on implicit and intangible knowledge, and, indeed, allowing machines to judge what research is appropriate opens new risks for both scholarship and practice. With such developments, if mentoring around a wide range of scholarly skills becomes redundant, traditions of scholarly knowledge sustenance may be lost (Gendron et al., 2022); the latter may be hard to rebuild if the impacts of a yet largely unproven technology prove counterproductive or even dangerous.

The ongoing advancements and discussions surrounding generative AI indicate its capacity to transform HRM processes, functions and activities. It is crucial to comprehend how organisations can harness its power to create value. We believe HRM research can offer important insights to critically examine and understand the implications of this new generation of ML models. By doing so, HRM can be driving force guiding responsible and purposeful adoption of generative AI which will lead to valuable HR outcomes. Considering the innovation

# BUDHWAR et al.

and disruptive capabilities of generative AI, and its positive and negative consequences, we consolidate 11 perspectives to shed light on advancing HRM scholarship in this area. These perspectives aim to uncover unexplored research avenues that will guide HRM scholars to conduct future research, with the goal of understanding and guiding the responsible and ethical implementation of this technology in the society. The remainder of this perspectives editorial is organised as follows: Section 2 offers historical perspectives on the business implications of generative AI, including an open resource-based view on its integration in HRM and its impact on HRM-related outcomes. Section 3 focuses on employee relations, well-being, talent management, and performance management. In Section 4, a range of research opportunities is presented, covering International HRM, equity, diversity and inclusion, and sustainable HRM. Section 5 provides insights and recommendations for utilizing generative AI in academic research. Section 6 engages in a discussion of common themes emerging from the perspectives, analysing the implications of generative AI on the economy, society, and HRM. Lastly, Section 7 offers our concluding remarks.

The full list of scholars and their critical perspectives on ChatGPT and its impact on HRM are listed below.

# Section 2: Generative AI Business Implications and HRM Outcomes

- What can we Learn about Disruptive Technology and Business Implications from Business History? (Stephanie Decker and Savvas Papagiannidis)
- Open Resource-based View – ChatGPT and Strategy to Deploy it within HRM (Jaap Paauwe and Paul Boselie)
- The Implications of Generative Artificial Intelligence for HRM-Related Outcomes: Analysis and Research Implications (David Guest and Vijay Pereira)

# Section 3: Generative AI – Employee Relations, Wellbeing, Talent and Performance Management

- Employment Relations, Employee wellbeing and Engagement (Andrew J. Knoblich and Steven Rogelberg)
- What are Implications and Challenges of ChatGPT and other Generative AI-Driven Tools for Employment Relations? (Greg J Bamber and Ashish Malik)
- ChatGPT Talent Management and Advising Managers on Performance Management. (Arup Varma and Angelo DeNisi)

# Section 4: Generative AI – IHRM, EDI and Sustainable HRM

- The Promises and Perils of Generative AI: An International Human Resource Management Research Agenda (Rosalie L. Tung)
- ChatGPT, EDI and Implications for HRM (Fang Lee Cooke and Charmi Patel)
- Generative AI and Sustainable Human Resource Management (Shuang Ren and Prasanta Kumar Dey)

# Section 5: Generative AI and Academic Research Methods

- Insights and Recommendations from a Research Methods' Conversation with ChatGPT (Mark NK Saunders)
- Using Generative AI as a Methodology Assistant: Trust, but Verify (Herman Aguinis and Jose R. Beltran)

# 2 GENERATIVE AI BUSINESS IMPLICATIONS AND HRM OUTCOMES

# 2.1 What can we learn about Disruptive Technology and Business Implications from Business History?

Stephanie Decker and Savvas Papagiannidis

Generative AI has prompted a variety of responses from users since Chat-GPT launched. Previously, the remit of experts, LLMs are now the focus of a wider public debate as to the future of work and the value of human labour more

BUDHWAR et al. 9

broadly. Anxieties around revolutionary new technologies are not new – when steam trains began as a form of passenger transport, concerns were raised about the high speed potentially rendering passengers mad (Milne-Smith, 2016). Mobile phones similarly generated health concerns due to potential invisible radiation (FDA., 2022) or indeed the possibility of deformed fingers from mobile phone overuse – the “smartphone pinky” (Taylor, 2020). But overly boosterish evaluations of new technologies often turn out to be equally overblown. For example, 3D-printing technology was invented over 40 years ago but is only now seeing wider commercial adoption (PWC., 2016), and so-called MOOCs (Massive Open Online Courses) did not significantly challenge or reform the provision of higher education globally (Borden, 2014). For every example from history of a disruptive technology, it is equally possible to find a counterexample of hype without much subsequent effect. What is clear is that technology by itself does not disrupt business and society, but that only does so in conjunction with efficiency gains, setting of new standards and business model innovation (Cortada, 2006, 2013). And as the history of dot-com boom and bust has shown, understanding how to commercially exploit a technology can be more difficult than mastering it (Decker et al., 2022).

While the debate about the role of AI in our professional, educational and personal lives rages on, we do not currently know how pervasive and commercially viable generative AI will become. Will it become a semi-public good, with most people being able to access AI tools either for free or in relatively affordable SaaS (software-as-a-service) packages, or, after its initial introduction, will the highest-performing AI models be so costly as to remain the preserve of large well-funded companies that deploy it with little oversight or transparency? At present, it looks as if AI will become ubiquitous. For instance, Microsoft's plans to introduce Copilot, an AI assistant in its Office Suite will see the technology becoming instantly available to hundreds of millions of users worldwide (Spataro, 2023). Integrating generative AI into widely adopted productivity suites will be a catalytic factor for meeting en masse the two key factors of technology acceptance, namely perceived useful and perceived ease of use (Davis, 1989). In turn this could normalise its use and influence how perceptions and future policies related to generative AI are set.

But an even more fundamental question is whether we will see AI replacing human labour and decisions, or playing a largely assistive role, facilitating human decision-making in the face of information overload. Both options stand to create substantial economic gains, and a recent Goldman Sachs report (Hatzius et al., 2023) predicted that generative AI could raise annual US labour productivity growth by just under 1½pp over a 10-year period following widespread adoption. Such expectations are dependent on the level of difficulty that AI will be able to perform and the number of jobs that are ultimately automated, suggesting that the most optimistic of these scenarios assume that a wide range of jobs currently performed by humans will be rendered obsolete. Replacement by and collaboration in automated processes have clearly been drivers of substantial productivity gains since the Industrial Revolution. Charlie Chaplin's “Modern Times” humorously portrayed this human subservience to machine; at the end of the 20th and early 21st century, James Cameron's Terminator series took this anxiety of being enslaved by machines to its logical conclusion. Economic historians have pointed out that significant productivity gains through technology depend on a range of factors (Crafts, 2010) and that large productivity gains have previously been most significant in manufacturing, with the service sector often seeing more modest gains (Bosworth & Collins, 2008). Yet AI tools may provide a step change in this regard, as large language models have the potential to rapidly displace humans in the services sectors. And with most contemporary developed economies strongly skewed towards the services sector, the impact on service and even professional employment could be profound.

Even where jobs may not be displaced, they may be degraded. AI can increase productivity, but lower (what is often already low) our ability for critical thinking. Direct comparisons of AI with humans can result in unfounded comfort that we are “better” than AI. For instance, we are worried that AI can provide wrong answers to questions, as if humans only provide correct answers. Similarly, we are concerned that AI can be biased, as if humans are not. LLMs have been trained on human conversation and writing on the Internet, and they absorb the biases that we as societies had already reproduced digitally. They are also facilitated by the profound inequities in the global economy, with AI workers in African countries providing the bulk of low-cost human labour involved in training the LLMs (Perrigo, 2023). Rather than counter-acting our biases, it may compound them. More optimistically, it may hold up the mirror to our own limitations. But this would require a critical engagement with how AI models work. However,

BUDHWAR et al.

calls for “explainable AI” (Gilpin et al., 2018; The Royal Society, 2019) may underestimate the complexity necessary for AI models to be high functioning. If we increasingly rely on AI in the future, this may have an impact on our ability to think critically and independently. Without the necessary knowledge, skills and experience we will not be able to question AI-based decisions and may just go along to hide our ignorance. This may then create a feedback loop. The more we trust AI, the more the scope of challenging it will become smaller. Such a challenge will not just affect those in relatively low-level and monotonous jobs, but everyone across the organisation.

Assuming widespread adoption of AI so that organisations can remain competitive, relying on AI can result in homogenous work outputs that are likely to be devalued over time. On the one hand the easier something becomes to produce, especially when humans are not necessarily a key part of the production any longer, the lower the value. On the other, what is the relative advantage when lowering the barriers for generating outputs means that similar or even effectively the same output is generated by everyone else? This is especially true if there is no diversification in how AI systems are developed and managed and we end up with a handful of AI providers/systems available to end-users. Ironically, this may end up increasing the perceived value of human-generated outputs that will become premium. We are already seeing this with the growth of craft industries and products, such as the ubiquitous rise of micro-breweries that combine “old” craft knowledge with the accessibility of cheaper modern brewing equipment (and tax breaks in many countries) (Bell et al., 2018, 2021; Lamertz et al., 2016; Suddaby et al., 2017). On the other hand, humans will never be able to follow the machines' pace. Even if the quality of the work is considered premium, the volume with which this is generated will never be on par. Neither can human response meet the ability of the machine to produce outputs on the spot. As such human contributions at the task level are likely to be devalued by their managers creating a cascading effect. This will have repercussions for both undertaking tasks, but also supervision and management.

History does not lend itself to predicting the future – for every industrial revolution there are the difficult to explain and much debated alternative trajectories (Pomeranz, 2009). AI is now poised to have a significant impact on our economies, our working and even our personal lives. What has barely featured in our current discussions of AI are its environmental costs – large language models currently run in massive data centres drawing significant amounts of energy (Bender et al., 2021). Industrialisation may have brought significant increases in wealth and welfare historically, but it has undoubtedly also contributed to the increase in resource use that is driving the current climate emergency (McNeill, 2011). From the sustainability perspective, AI continues our current trajectory of massively escalating our per capita energy use, with all the environmental degradation that entails.

# 2.1.1 Research agenda

Historical research may help us better understand how societies, organizations and employees respond to the transformations wrought by technology – from the Luddites destroying machines in the nineteenth century (Thompson, 1966) to the Dot-com boom and bust at the turn of the 21st century (Goldfarb & Kirsch, 2019).

- What are the factors that determine how fundamentally a technology disrupts business models and professionals' identities?
- As AI tools are becoming more ubiquitous in businesses and workplace, how will employees integrate these tools and ‘co-pilots’ into their work?
- Even now, the predecessor to Google's BARD, the BERT algorithm, is freely available and has been used to support information gathering and analysis (Venkata et al., 2021). Will AI be used entrepreneurially by individuals who tailor them for their specific professional uses, or will AI remain under the control of large corporations?
- How will AI adoption affect value perceptions of the outputs produced?

From the perspective of organisations and their employees, it remains difficult to say whether AI will have positive effects (enabling individuals to be more productive, with greater scope for personal development) or negative

BUDHWAR et al. 11

outcomes (reducing the scope for human decision-making and creativity, coupled with ever greater psychological pressures from increased expectations for productivity and performance). Very likely the effects will be felt differently across organizations and roles, but it will certainly have an impact on most people in some form. Generative AI systems like GPT-4 are still in their early days. Performance of these systems is close to that of humans and one could argue that they could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence system (Bubeck et al., 2023). AI does not have to become “perfect”, it only must become “better” than us and in many ways it is already.

# 2.2 Open Resource Based View – ChatGPT and strategy to deploy it within HRM

Jaap Paauwe10 and Paul Boselie11

# 2.2.1 Introduction

Contemporary strategic human resource management (SHRM) builds on human capital and resource-based view (RBV) notions that given the right circumstances employees (human resources) can be a source for sustained competitive advantage (Boon et al., 2018). Digitalization and technological developments such as ChatGPT can have disruptive effects and even cause a Schumpeter shock comparable to the development of digital photography in contrast to mainstream photo film rolls in the 90s. Disruptive technological developments and Schumpeter shocks are described in Barney's (1991) classic RBV article as phenomena that break through sustained competitive advantage positions of organizations. To what extent is ChatGPT a disruptive technology? And what is the impact of ChatGPT on strategic human resource management (SHRM) and HRM professionals? The application of ChatGPT in the SHRM domain can be found in, for example, writing policy documents on long term absence due to illness, recruitment and selection, onboarding processes, screening of job applicants and job applications, and constructing job descriptions (Renkema, 2023; Renkema et al., 2022). One of the interesting issues raised by Renkema (2023) refers to the question posed by Susskind and Susskind (2022): How is new HRM knowledge developed if HRM professionals are no longer necessary and replaced by technology? Replacement, substitution, value, imitation, and scarcity in HRM almost automatically make us think about SHRM and the resource-based view. Chaudhary (2023) summarizes a few areas where ChatGPT can improve HR processes including improved cost-effective solutions; easier HR data management and analytics; enhanced employee experience based on survey input; and simplified recruiting and onboarding. In this overview we focus on the possible negative and positive effects of ChatGPT using the classic RBV (Paauwe & Boselie, 2003) and an alternative theoretical framework we call: Open resource-based view to SHRM1. The overall aim is to provide a research agenda for studying SHRM and HRM professional implications from a uniqueness perspective.

# 2.2.2 Resource based view: The negative perspective

Reflecting on the past 30 years of research, the resource-based view is one of the most dominant theories in SHRM (Paauwe, 2004). Looking at the implications of widely applied ChatGPT soon, we foresee a severe reduction of the possibilities to achieve a sustained competitive advantage through SHRM. After all, considering the four conditions, as formulated by Barney (1991) they will lose their relevance:

- Value: ChatGPT is freely available for everyone, so there is no or limited possibility to generate and reap the rents as proprietary rights are lacking. The rents and the benefits are there for everybody, who knows how to handle and apply the software.
- Scarcity: Not anymore. This is not only the case for ChatGPT, but increasingly for all kinds of open-source software.
- Imitation: This used to be the core of the RBV theory and especially for achieving a ‘competitive advantage through people’ (Pfeffer, 1994), and clarified by the conditions of ‘causal ambiguity’, ‘path dependency’ and ‘social

# 2.2.3 | Open resource-based view: The positive perspective

In practice we are heading in the direction of increasingly openly available software, generating advantages and cost savings for all organizations, as well as for available workers for example, in areas of self-employment and the Gig economy (through digital platforms). Employee capabilities will be freely available and will no longer be protected by the firm in their role as employer through all kind of non-competitive and non-disclosure conditions.

Let us first sum up some advantages from an HR-function perspective. ChatGPT can be very helpful in improving cost-effectiveness within HR. Think of activities such as composing job descriptions, screening of applications based on job requirements, preparing semi-structured interview questions, developing training programs and -materials such as course outlines and onboarding instructions. Also, the writing of policy documents such as those related to absenteeism, hybrid working, improving employee engagement can be ‘outsourced’ to ChatGPT, without incurring any additional costs and saving a lot of manpower hours and expensive rates from consultancy firms. This transformation could lead to temporarily competitive advantage of the first moving organizations and individuals.

The ‘outsourcing’ of HRM practices and HRM activities is not completely without risks, with respect to issues of:

- Inaccurate or misleading information: Human judgement cannot be missed, as using ChatGPT might lead to misleading, outdated or wrong information.
- ChatGPT and context: Neglecting or overlooking the context, might lead to a lack of contextual understanding of the specific business, with all kinds of negative consequences for the right answers and effective decision-making.
- Outdated information: ChatGPT is built and trained on existing data. This implies that actual and future developments have not been considered. It is unclear to what extent ChatGPT and artificial intelligence in general can generate new insights and information by itself.
- Bias and discrimination: Experiences from large firms, which reflect on their recruitment efforts using artificial intelligence or ChatGPT, warn for racial bias and a tendency against female candidates.
- Privacy, Data security and GDPR: Data shared with the chatbot could potentially be exposed to unauthorized third parties or misused, resulting in data breaches or privacy violations.
- Legal and ethical concerns: What are the legal and ethical implications of using artificial intelligence chatbots? Who owns the IP of generated content? Will there be national laws or sectoral regulations, which hinder or forbid the use of ChatGPT?

The summary of these potential risks can be considered ‘critical conditions’ for an alternative resource-based view approach to studying the impact of ChatGPT on SHRM. Increasingly openly available resources at the level of the individual, the organization and societal level will enable and intensify collaborative innovation and will allow individuals, organizations, and society at large to generate rents available to all. In this way we are heading in the direction of an open resource-based view of the firm to strategic human resource management being able to generate employee.

BUDHWAR et al. 13

well-being, organizational efficiency, and societal well-being in line with the ultimate business goals or long-term consequences of the Harvard model (Beer et al., 1984):

- Organizational effectiveness can be achieved through efficiency improvements and more focus on HRM services delivery given ChatGPT's ability to take over all kinds of HRM practices and activities of HRM professionals and line managers.
- Employee well-being in multiple ways including (a) increased information availability, (b) direct knowledge sharing and (c) instant employee involvement through ChatGPT – human interface.
- Societal well-being is achieved through open and broad sharing of information through ChatGPT.

An open resource-based view using ChatGPT can contribute to HRM knowledge sharing and knowledge circulation between organizations and between individuals including HRM professionals. Cocreation, knowledge sharing and knowledge circulation between different organizations could be an alternative way for achieving competitive advantage of the organizations involved (a multi-level and multi-sector perspective). Open resources can be part of collaborative innovation and even co-opetition (cooperation in a competitive environment), because collaboration and knowledge exchange can be beneficial to the population in an era that individual organizations are not always capable of solving big societal and organizational challenges. The ‘classic’ resource-based view is very much focussed on the sustained competitive advantage of an individual organization. Our alternative open resource-based view to SHRM is aimed at the sustained benefits for organizations, individuals (employees and citizens) and society. In addition, using an open resource based theoretical perspective can still contribute to organizational sustained competitive advantage because of uniqueness through strategic focus of the HR function on specific themes that matter for the organization including the relevance of organizational context.

Strategic decision makers and HRM professionals can focus on HRM themes that matter and need special attention such as: building and changing organizational culture; creating a degree of psychological safety; contributing to the quality of collaboration both inside and outside with institutions, contractors, clients and suppliers; and positively affecting the degree of agility. Instead of the numerous day-to-day HRM activities (such as screening applicants and writing manuals) that are usually on the table, the application of ChatGPT on multiple HRM issues creates leeway for strategic focus on more complex and sophisticated organizational issues. Uniqueness lies in the fact that HRM professionals can focus on the things that really matter but are also difficult to dehumanize through ChatGPT. The very fact that HR professionals know both the internal and external context of their organization will enable them to adapt and customize the Chat GPT generated insights and policy texts in line with the unique context of their own organization. This implies that for HR professionals - despite openly available (re) sources - are still able to provide the organization with unique and thus organization-specific knowledge.

# 2.2.4 Research agenda

We suggest a further exploration of the open resource-based view of the firm using the threefold long-term consequences – organizational effectiveness, employee well-being and societal well-being – proposed by the Harvard model to study the impact of ChatGPT on SHRM. In doing so we must first clarify the theoretical foundations, assumptions, and critical conditions of this alternative theoretical framework. Building on the threefold long-term consequences we are thinking about the following research agenda:

- Collaborative innovation through ChatGPT and the impact on organizational performance in a sector or population using multiple case-study research design.
- HRM function transformations through the application of ChatGPT in terms of both HRM practices and the HR roles and the remaining leeway for uniqueness and thus contributing to a sustained competitive advantage through SHRM.

# 2.3 The Implications of Generative Artificial Intelligence for HRM-Related outcomes: Analysis and Research Implications

David Guest 16 and Vijay Pereira 21

Generative AI is the latest in a stream of developments in AI that have been characterised as reflecting a paradigm change and a distinctive stage of industrial development sometimes known as Industry 4.0. It is distinctive because it shifts the source and potentially the control of knowledge from people to machines. This has implications for management, workers and society at large to the extent that there have been calls to pause development while these implications are more fully considered (Shet & Pereira, 2021). This suggests the likely impact and outcomes of generative AI in the workplace and beyond are uncertain. It therefore offers a rich new research agenda for HRM scholars since it has the potential to affect many features of the content, processes and outcomes of HRM activity (see Pereira et al., 2023). While these elements of HRM are interrelated, in this discussion, we will focus on consideration of potential outcomes.

The central feature of generative AI, reflected for example, in ChatGPT and Bard, is the capacity to store limitless amounts of knowledge and to present it in a coherent, useable form. It can undertake a range of activities that previously required human input. It therefore offers an attractive proposition to industry by promising availability of analysed information of quality in great quantity and at much greater speed than humans can achieve.

For HRM researchers exploring outcomes of generative AI, we suggest two analytic frameworks. The first is the stakeholder approach of Beer et al., 1984 who propose that HRM should pursue the core goals of organizational effectiveness, employee well-being and societal well-being while taking into consideration a range of stakeholders such as management, shareholders, employees, government, the community and unions. One reason for this is a view that to date AI has been mainly used to promote efficiency and flexibility in the interests of shareholders at the expense of other stakeholders. Generative AI has the potential to have an impact well beyond the workplace.

The second and complementary analytic framework is found in the advocacy of Industry 5.0. Among academics, among governments such as in Japan as well as in the European Commission (Breque et al., 2021) there is a recognition that the potentially dramatic impact of Industry 4.0, including generative AI, requires a rebalancing to ensure that AI works in the interests of all at work and in society reflected in outcomes consonant with improving the quality of working life and the quality of life for citizens as a whole (Guest et al., 2022). This has led to calls for a revitalisation of a socio-technical systems perspective to ensure that developments are sufficiently human-centric (Bednar & Welch, 2020). A feature of the socio-technical approach is the recognition of organizational choice in the design of work and through this the potential to address the important outcomes of the various stakeholders (Trist et al., 1963).

With these analytic considerations in mind, we can identify some outcomes that can provide a focus for research on generative AI as it affects the core outcomes identified by Beer et al., 1984 namely organizational effectiveness, employee well-being and societal well-being. In doing so, we will consider both positive and negative potential outcomes and, mindful of the specific HR function responsibility for oversight of people management, address the features associated with Industry 5.0.

In the past, organizations have sought to gain competitive advantage by embracing new technology. There is no reason to believe that it will be any different with AI and specifically with generative AI. Seen through

# BUDHWAR et al.

the theoretical lens of the Knowledge Based View (KBV), a specific feature of the resource-based view of the firm (Barney, 1991), we primarily perceive that organizations possess unique bundles of idiosyncratic knowledge capabilities and resources (knowledge pertaining to generative AI), that need to be managed to maximize value through its optimal deployment, whilst identifying and developing its knowledge capability and resource base for the future (Grant, 1996). Thus, the HR function can use generative AI to improve its own processes and outcomes and contribute to organizational effectiveness through more efficient processing and decision-making in a range of HR practices. These potentially include swift provision of semi-standardised information in areas such as job descriptions, job profiles, employment forms and procedures. The interactive nature of bots (short for robots) offers considerable potential for training and development and can undertake activities such as addressing employee queries currently provided by HR call centres. For some time there has been advocacy of evidence based HRM. A potentially helpful outcome of generative AI is the availability of evidence-based information on which to base HR decisions and improve HR outcomes. This can address both external evidence, drawing on systematic reviews of the impact of policy and other interventions to improve people management, and internal HR analytic data to provide analysis of labour turnover, absence and much HR data. Reducing the time to assemble HR information could release time for core HR activities including supporting line management, addressing employee problems and contributing to strategic thinking about the kind of outcomes to prioritise for people management. This is the promise; the reality may be more challenging with questions about the trustworthiness of AI-generated data, confidence among employees in interacting with bots, and imbalance in the socio-technical system.

The potential outcomes of generative AI for employee well-being are likely to attract considerable attention. In the post-Covid era, there is already considerable concern about the health of the workforce and organizations ranging from the OECD to the UK Chartered Institute of Personnel and development have called for improvements in the quality of work (see Guest et al., 2022). This focus should provide a core part of an evaluation of outcomes. More specifically, evaluation might focus on changes in the configuration of roles and on changes in autonomy and decision-making. Positive outcomes could include opportunities for many in knowledge, technical and professional jobs to reduce the routine activities releasing time for the skills where human input will be superior including judgement, innovation, problem-solving and interpersonal interaction. There will also be a continuing and expanding need for many skill-based jobs. However, there is a risk that reconfiguring work also leads to the loss of several routine administrative jobs presenting challenges for the HR function as it manages the change in the composition of the workforce. There is also the risk, outlined above, that management encourages outcomes that result in technological determinism allied to a lack of effective monitoring of information and decisions produced by generative AI.

The outcomes in terms of community well-being point to a need to prioritise the development of relevant skills to address the risk of a growing divide between those who are more or less employable. A challenge for the HR function will be how to contribute to re-skilling of those both in their organization's employment and a responsibility to support skill development through systems such as apprenticeships and retraining. This is especially so, given the prediction of job losses or degrades, due to AI, to the tune of 300 million, by a recent Goldman Sachs report, as reported by Forbes (Kelly, 2023). In the wider society, the way in which generative AI is applied in organizations will affect perceptions of fairness, inequality and trust in management.

In summary, generative AI creates a new research agenda for HRM scholars. A core research question in evaluating the outcomes of AI concerns the extent to which a successful balance has been achieved between the technical and the social systems to ensure that the needs of stakeholders are addressed and that it leads to a good quality of working life with spill over effects into the community and beyond. This outcome is only likely if there is a real possibility of mutual gains for the main stakeholders; indeed, it extends the agenda for research on mutual gains. It also provides the HRM community and the HR function with a distinctive advocacy role for a human-centric AI and in particular generative AI; and this provides the core focus for evaluation of its outcomes.

# 3 | GENERATIVE AI – EMPLOYEE RELATIONS, WELLBEING, TALENT AND PERFORMANCE MANAGEMENT

# 3.1 | ChatGPT impact on employment relations, employee wellbeing and engagement

Andrew J. Knoblich17 and Steven Rogelberg17

# 3.1.1 | Introduction

Organizations are increasingly adopting AI-enabled chatbots and conversational agents, like ChatGPT, to enhance stakeholder relations, engagement, and wellbeing across various industries (McKinsey Global Institute, 2017). Whereas ChatGPT's impact across contexts will vary, this technology is poised to substantially alter HRM practices and systems in most organizational settings (Edlich et al., 2018; OpenAI, 2023). In its present state, ChatGPT can be integrated into work environments as a component of a bespoke network of applications (OpenAI, 2023). Depending on its use case, ChatGPT can provide on-demand personalized support, novel responses to complex queries, and guidance on a host of topics (Hatzius et al., 2023). The purpose of this section is to discuss the impact of ChatGPT on employment relations, employee wellbeing and employee engagement as well as to outline a research agenda for further investigation.

# 3.1.2 | Implications of ChatGPT

# Employment Relations

ChatGPT has the potential to automate various commonplace tasks in the HRM context. For example, Edlich et al. (2018) highlighted the incorporation of chatbots and cognitive agents in HR service centres that responded to employee inquiries, managed benefits administration, and oversaw record-keeping. In this given context, employees can benefit from on-demand access to information, resources, and services. Additionally, the resulting analytics can help drive better decision-making of leaders and yield improved HRM support thereby impacting employment relations. An important variable that is likely to moderate employment relations using ChatGPT-enabled interfaces is trust.

Given that trust is a central factor in the employee-employer relationship (Yadav et al., 2022), employment relations is likely to be moderated through the use of ChatGPT interfaces insomuch as employees view the technology as an appropriately leveraged, capable, and trusted resource. To explore some of its limitations, Figure 2 exhibits ChatGPT’s response to an inquiry, “Can I trust you?” In response, the model indicates several of its limitations.

# Employee Wellbeing

Another notable example of an AI-enabled chatbot is Woebot, a commercially available conversational agent that uses cognitive-behavioural therapy to provide mental health support to its users (Fitzpatrick et al., 2017). During a randomized control trial, participants in the treatment group (which used Woebot as a therapeutic intervention for 2 weeks) reported a significant reduction in depressive symptoms (Fitzpatrick et al., 2017). While Woebot is not a ChatGPT-enabled application, its function is illustrative of the potential impact ChatGPT can have on employee wellbeing, as it relates to mental health. Figure 3 illustrates ChatGPT's response to the prompt, “I feel depressed.” The model first expresses empathy. Then it notes its limitations and subsequently offers general recommendations.

Conversely, the integration of ChatGPT into the workplace may also negatively affect employee wellbeing. For instance, the integration of ChatGPT may increase anxiety for employees concerned about job insecurity (OpenAI, 2023). Figure 4 illustrates ChatGPT's response to the user prompt, “I am worried about how the integration of ChatGPT will lead to the loss of my job.” Again, the model expresses empathy before offering suggestions.

# BUDHWAR et al.

# Can I trust you?

As an AI language model, I am designed to provide information and answer questions to the best of my ability based on the data I have been trained on. However, there are some important considerations to keep in mind when using AI like me:

1. Knowledge limitations: My knowledge is based on the data I was trained on, which goes up until September 2021. I may not have the most up-to-date information, and my knowledge of specific subjects might be limited.
2. Misinterpretation: I might sometimes misunderstand your questions or provide answers that don't fully address your concerns. This can be due to the complexity of language and the limitations of AI understanding.
3. Bias: Although efforts have been made to minimize bias in my training data, I might still inadvertently display some biases present in the text I have been trained on.
4. Not a substitute for professionals: While I can provide general information and advice on various topics, I am not a substitute for consulting with professionals, such as doctors, lawyers, or financial advisors.

You can trust me to provide general information and answer questions to the best of my ability; but it's always a good idea to verify information, especially when it comes to critical decisions or matters that require expert guidance.

# FIGURE 2

ChatGPT response to prompt regarding trust.

# Employee Engagement

Like employee wellbeing, ChatGPT may likely influence employee engagement both positively and negatively. AI and generative models like ChatGPT can enhance the efforts of human workers in a myriad of ways ranging from upskilling in call centres to providing help with writing to providing on-demand resources (Cheng et al., 2022; OpenAI, 2023). The outcomes of this AI augmentation can be positive for workers as they may yield opportunities for empowerment, creative exploration, increased efficiency, and fulfilment. These outcomes, in turn, may enhance motivation and overall job satisfaction. Researchers have established that perceived satisfaction is related to engagement (Cheng et al., 2022). In this way, the adoption of ChatGPT has the potential to enhance employee engagement.

However, OpenAI (2023) has called attention to the possibility of increased inequality and disparate impact that may result from the use of ChatGPT. The possibility of job displacement or the redefinition of roles may differentially impact employees. Employees may also struggle to adapt to new processes. These challenges may result in employees' wasted time, feelings of frustration, and distrust. In this way, the use of ChatGPT can be expected to negatively impact employee engagement.

feel depressed

I'm sorry to hear that you're feeling depressed: It's important to remember that I am an AI language model and not a mental health professional; but I can still offer some general suggestions that might help:

1. Reach out to someone: Talk to a friend, family member, or mental health professional about your feelings. Sometimes just sharing your thoughts with someone you trust can provide relief.
2. Develop a routine: Create a daily schedule that includes time for work, self-care, and leisure. Maintaining a routine can provide structure and help you feel more in control of your life.
3. Engage in physical activity: Regular exercise can boost your mood and help relieve stress. Even just going for a walk can be beneficial.
4. Practice mindfulness: Techniques like meditation, deep breathing, and progressive muscle relaxation can help you become more aware of your thoughts and feelings and make it easier to cope with them.
5. Seek professional help: If your depression is severe or persistent, it's important to consult with a mental health professional who can provide guidance and support tailored to your specific needs.

Remember, it's crucial to consult with a mental health professional for personalized advice and appropriate treatment. Don't hesitate to reach out to a therapist, psychologist, or psychiatrist if you're struggling with depression:

FIGURE 3 ChatGPT response to expressed human depressive state.

# 3.1.3 Research agenda

Given that ChatGPT's adoption in organizations is likely to transform HRM practices and systems, with both positive and negative implications for employment relations, employee wellbeing, and engagement, extensive research is needed on the subject. The following research agenda is likely to elucidate latent knowledge on ChatGPT.

# 3.1.4 Research on Stakeholder Perceptions and Adoption of ChatGPT

Cheng et al., 2022, noted a low-level of consumer intention to accept and use chatbots due to frequent failures in human-chatbot interactions. A 2022 IPSOS survey found varying levels of agreement with the statement “products and services using AI had more benefits than drawbacks” (Maslej et al., 2023, p. 322). Brandtzaeg and Følstad (2017) further examined the various motivations of using chatbots. Thus, we propose the following research questions:

- How do organizational stakeholders perceive the use of ChatGPT in the workplace, and what factors influence their perceptions?

BUDHWAR et al. 19

I am worried about how the integration of ChatGPT will lead to the loss of my job: It's understandable to be concerned about job security in the face of new technologies like ChatGPT. While AI advancements can indeed lead to job displacement in some sectors, it's important to remember that they also have the potential to create new job opportunities and enhance existing roles.

Here are a few suggestions on how to adapt and thrive in this changing landscape:

1. Embrace lifelong learning: Continuously update your skillset and stay informed about advancements in your field. Consider taking online courses or attending workshops to acquire new skills that complement AI technology.
2. Focus on human skills: Develop and hone your soft skills, such as empathy, critical thinking, creativity, and communication, which are less likely to be replaced by AI.
3. Adaptability: Be open to change and adapt to new ways of working. Embrace new technologies and learn how to use them to your advantage.
4. Network: Build and maintain connections with professionals in your field, as well as those in related industries. Networking can open doors to new opportunities and help you stay informed about the latest trends.
5. Look for new opportunities: AI is likely to create new job roles and industries. Keep an eye out for these emerging fields and consider whether they align with your skills and interests.

Remember, the key to thriving in the face of technological advancements is to remain adaptable, proactive, and willing to learn. By focusing on developing your skills and staying informed, you can improve your chances of staying relevant and valuable in the job market.

FIGURE 4 ChatGPT response to expressed human concern of job loss.

- What factors contribute to the successful adoption of ChatGPT in the workplace, and how can organizations overcome potential barriers to adoption?
- How effective is ChatGPT in improving communication, engagement, and wellbeing of employees in various work environments and industries?
- Which skills and characteristics remain distinctive and essential to human employees?

# 3.1.5 Research on the Safety, Ethicality, and Equity Considerations of ChatGPT

OpenAI (2023) highlighted several issues with its most recent model, GPT-4. Notably, GPT-4 produced biased and unreliable content, could provide information on how to sell illicit drugs and services, and was able to support efforts to plan violent attacks (OpenAI, 2023). Even considering the myriad of benefits this technology may have, it is

# 3.2 What are Implications and Challenges of ChatGPT and other generative AI-Driven Tools for Employment Relations?

Greg J. Bamber 8,9,10 and Ashish Malik18

Many scholars conduct HRM research using a unitary frame of reference. By contrast, most Employment Relations (ER) scholars conduct research using pluralist or radical frames of reference. It is important to recognize different frames for understanding HRM and ER outcomes and conflicts (Budd et al., 2022; Teicher et al., 2023). The ER field includes many aspects of people at work and takes in the interactions, job regulation, collective bargaining, conflicting interests and power struggles between employers and employees and their representatives, such as lawyers, managers, employers' associations and unions. ER also includes the roles of “third parties” such as arbitrators, policymakers, government agencies and other stakeholders (Bamber et al., 2021). Fairness, accuracy, transparency and trust are important aspects of ER. In this context, what, then, are the implications and uses of ChatGPT and other generative AI-driven tools for ER?

ChatGPT-like innovations use generative AI and LLM employing ML technologies to deliver human-like conversational interactions. These disruptive AI-driven tools have precipitated much discussion and emerging research agendas regarding the implications for managers and those trying to represent, educate and train workers (Dwivedi et al., 2023). Such disruptions have sparked interest in how such AI-driven tools may affect workplaces (Milanez, 2023) and ER stakeholders (Ioakimidis & Maglajlic, 2023; Zu, 2023) and HRM (Korzynski et al., 2023).

As developments in this domain are moving rapidly, we highlight some initial implications of such AI-driven tools for ER. Stakeholders can deploy these innovations to:

- generate possible scenarios and personalised and individualised training solutions for employees to address the current and foreseeable skill gaps created by AI applications (Tcharnetsky & Vogt, 2023).
- produce narratives that may enhance employees' education and training.
- automate some tasks that involve language processing, such as reviewing employment-related contracts, and other documents.
- create draft workplace communications and rules regarding protocols and safety.
- analyse and share HR analytic data.
- synthesise employees' voice channels, suggestions, feedback, and insights.

# Research Questions

- What are the potential privacy and security risks associated with using ChatGPT in the workplace, and how can organizations mitigate these risks?
- What are the ethical considerations associated with using ChatGPT in the workplace, and how can organizations ensure that the use of ChatGPT is aligned with ethical standards and guidelines?
- How does the use of ChatGPT impact the organizational culture, and how can organizations ensure that ChatGPT aligns with their values and culture?
- What are the long-term effects of using ChatGPT in the workplace, and how can organizations measure and evaluate these effects?

This research agenda does not reflect an exhaustive list of research considerations. However, given the present—and impending exponential—adoption of this type of technology in the workplace, these inquiries may illuminate best practices, benefits, risks, and outcomes associated with HRM practices and the impact on employment relations, employee wellbeing and engagement.

# BUDHWAR et al.

• increase awareness of issues around inequality, transparency and legal frameworks.

• spawn opportunities for new analytical, ethics, data science and responsible AI-related jobs.

In collective bargaining, some negotiators could use these innovations to facilitate their research, claim formulation and other preparations. For example, negotiators could use the ChatGPT-like innovations to help them research cases more widely and quickly than they could otherwise, for instance, to find and assemble precedents, develop arguments, and draft speeches. In addition, union leaders and officials can draw on these innovations to facilitate the recruitment and retention of members, and communications with them. Even though large technology firms' rhetoric may claim that these ChatGPT-like innovations promote empowerment and other advantages, there may also be disadvantages. Such technologies are not a panacea; they also have limitations, potential dark sides and challenges (van Dis et al., 2023). This is especially because employers may have moral and legal obligations and responsibilities towards employees, who employers should consult before making major changes to employment conditions. These obligations and responsibilities vary in different national and regional contexts (On the Australian context, see Clayton UTZ, 2023).

For example, using these innovations in inappropriate ways may:

- precipitate greater levels of job insecurity and increased job precarity, which often becomes a latent or explicit issue in ER disputes.
- lead to more unfairness in workplaces, especially as the users may not understand these innovations' limitations.
- produce harmful, misleading or wrong outputs that can damage trust in employers, unions and managers' reputations.
- create legal problems when used for ER decisions when there may be input mistakes or inaccurate output.
- raise ethical and legal issues related to intellectual property, privacy, consent, covert surveillance and accountability of employees' personal data.
- cause complex consent issues, for instance, due to the lack of transparency; for instance, if stakeholders were subsequently to use employees' data for reasons other than those specified in the original consent, especially if data enters an LLM's database.
- not always be fully in tune with the context, intent, nuance or tone of the user or the task and therefore produce inaccurate narratives.
- generate wrong, inappropriate or irrelevant responses.
- create new jobs that may require high-tech skills or be in the “gig economy”; most such jobs are not currently covered by unions.
- foster employees' psychological distress and decrease their sense of wellbeing.

Therefore, these innovations should be used thoughtfully and responsibly by considering the potential advantages and disadvantages for different audiences, stakeholders and scenarios. ChatGPT-like innovations should involve human oversight and intervention, using practices such as verification and independent moderation of the input data, process technologies and output selection bases. This is necessary to ensure quality, accuracy, compliance with the law, collective agreements/contracts, and ethical behaviour. Employees are likely to be alienated if they infer that they are being managed or represented by these technologies or algorithms. We hypothesise that most employees prefer to be managed and represented by people, with whom they can discuss matters, rather than by impersonal technologies or algorithms.

Given that many ChatGPT-like innovations will be developed further, affecting ER in varied ways, we encourage ER stakeholders to facilitate employees having a voice in how technologies are used. Depending on the context, this should involve managers consulting with unions or other representatives of employees. Technologies would generally be implemented more equitably, effectively and sustainably if the innovations include transparent co-design and co-regulation that involves employees and their representatives as stakeholders. This should be encouraged to counter the temptation for managers, technocrats, the global big tech firms and consultants to deploy these generative AI-driven tools unilaterally and secretly within a unitary frame of reference.

# 3.3 ChatGPT Talent Management and Advising Managers on Performance Management

Arup Varma25 and Angelo DeNisi14

The proliferation of Artificial Intelligence (AI) and AI enabled tools such as ChatGPT is a welcome development, but also a potential danger to our way of living, if left unharnessed. In this connection, an analysis by Swiss Bank had noted that ChatGPT is the fastest growing app of all time (see Ortiz, 2023). Not surprisingly scholars such as Prikshat et al. (2022) have noted, “AI-augmented HRM can improve workflow and productivity, reduce costs, and increase accuracy in HRM functions.” Indeed, the speed at which AI enabled tools such as ChatGPT are emerging, these applications can help organizations and societies in numerous ways. We believe that organizations that choose to ignore ChatGPT do so at their own peril.

One area where AI could make major inroads is the critical area of employee performance management given the breadth and scope and the continuing evolution of the function (see, e.g., DeNisi et al., 2021, 2023). Since the performance management function requires raters to make judgments based on copious amounts of qualitative and quantitative data, ChatGPT can prove very useful in terms of processing complex data. However, this is where the danger lies. Since AI enabled tools such as ChatGPT rely on the information available to it, organizations need to be very cautious and deliberate in what information is available to the bot.

A couple of recent examples help illustrate the need for caution. In one case, Sydney, an AI-enabled bot, tried to convince a New York Times writer to leave his spouse (see Wolf, 2023). In another equally bizarre outcome, Google's rival to ChatGPT, Bard, suggested that Google is a monopoly, and the government should step in and break it up (see Prakash, 2023). The point is that when it comes to using ChatGPT for performance management, organizations and human resource managers need to step up their game and monitor and control both formal information and informal chatter within their organizations. If HR managers are willing and able to step up and meet the ChatGPT challenge, they can use ChatGPT to their advantage, when it comes to performance management.

Indeed, we would like to urge HR practitioners to be proactive and help identify facets of performance management where the bot could help managers (who are typically the ones doing the ratings) do a better job. Recently, a company called Confirm (see Parisi, 2023) has experimented with using ChatGPT in their performance management process – specifically for performance reviews. Their experience has been promising – the bot was very successful in writing performance reviews by asking pointed questions of colleagues of the ratee. However, as the CEO of Confirm cautioned, ChatGPT is not ready to give feedback to individual employees, yet. We would hasten to add that the bot

BUDHWAR et al. 23

should probably not be giving feedback anytime soon, as performance management is a human process and should stay as such – meaning that the bot should assist individual managers in doing their jobs as supervisors, not replace them.

Similarly, one of the concerns often expressed by scholars of performance management (see DeNisi & Murphy, 2017) is the failure of performance management scholars and practitioners to establish a clear link between individual performance and organization-level outcomes. Perhaps ChatGPT could help address this issue by helping organizations map individual level performance with organizational outcomes through analysing performance-related information and investigating how the various pieces of individual performance fit in with organization level outcomes. Of course, as we know, currently this link is tenuous at best, with the intervening step being more of a black hole whereby information is fed, and outcomes decided though the mechanism through which the data are transformed into individual outcomes is questionable. We believe this presents a tremendous opportunity for scholars of performance management to lead the field proactively rather than simply conducting post-hoc studies.

Indeed, given that ChatGPT is in its nascent stage, there are several other facets of performance management that beg scholarly investigations. First, the level of comfort of the rater with technology and his/her willingness to incorporate ChatGPT in the PM process could determine the degree to which the bot can assist organizations. Next, it is well known that performance management is fertile ground for bias and bots such as these can exacerbate this problem. Indeed, there are studies that show that interacting with bots and other AI enabled tools increases the tendency of human beings to engage in unethical behaviours (see, e.g., Kim et al., 2022). Thus, the ethical aspects of the use of the bot should be investigated (see also, Varma, Dawkins, & Chaudhuri, 2022).

Relatedly, it is well-known that supervisor-subordinate relationships play a critical role in the determination of subordinate ratings and related outcomes (see, Varma, Jaiswal, et al., 2022). Given that most, if not all, PA systems do not specifically incorporate this factor in the PA process, scholars would do well to examine the flow, sources, and quantity of non-performance and performance-related information to see whether ChatGPT is able to separate relevant information from irrelevant information and the degree to which it bases its suggestions on performance related information. Finally, it would be important to examine the differences in perceptions of and reactions to ChatGPT between the different generations of employees within organizations.

In conclusion, we believe that ChatGPT has the potential to make raters' lives easier, but the bot needs to be proactively managed, and the users need to be trained in using the application appropriately.

# 4 | GENERATIVE AI – IHRM, EDI AND SUSTAINABLE HRM

# 4.1 | The Promises and Perils of Generative AI: An International Human Resource Management Research Agenda

Rosalie L. Tung24

Since ChatGPT's debut in November 2022, there have been widespread attention to and coverage of generative Artificial Intelligence (AI), one of two broad domains of AI. According to Berg (2023), generative AI is suited to the generation of “new content” while, discriminative AI is best suited “for analysis”. Much has been written on what ChatGPT can do and the far-reaching ramifications of such changes to virtually all aspects of social interactions and societal functioning, including the bright and dark sides associated with these developments. Coincidentally, ChatGPT was introduced at a time when people from around the world were just emerging from profound societal upheavals related to the covid pandemic – the “great resignation” or “quiet quitting” in many industrialized countries; the intensification of the war for talent worldwide exacerbated, in part at least, by strategic rivalry among countries, especially between the world's two largest economies to retain or attain primacy in the calculus of global competition on multiple fronts; the growing preference by some for remote work made possible, to a large extent, by technological developments, such as virtual teaching, meetings and conference participation; and the overall ageing of the

population in most industrialized countries and China, the so-called workshop of the world and the lead engine for economic growth worldwide in the past 4 decades. The overall ageing of the workforce compounds to the shortage of labour that many countries have been experiencing.

Like developments surrounding the previous rounds of Industrial Revolutions (Industry 1.0 represents the introduction of steam power through Industry 4.0 and Industry 5.0, or the introduction of smart devices and human-machine collaboration, respectively), initial concerns were raised about massive displacement of workers and their widespread impact on society, including economic growth, efficiency/productivity, wealth and the general well-being of its people. Fortunately, many of the worst-case scenarios associated with each iteration of the Industrial Revolutions failed to materialize. To the contrary, the benefits appeared to far outweigh the costs in each Industrial Revolution albeit each round was accompanied by profound changes that pervaded all aspects of HRM.

Like many others, I have followed with both excitement and trepidation the promises and perils of ChatGPT and have reflected on what they mean for human resource management theories, policies and practices. While these are manifold, I will identify and focus on two of them and briefly outline a research agenda associated with each.

# 4.1.1 The productivity paradox

A major concern of ChatGPT and AI is that it will replace the work that is currently done by assembly line workers and professionals alike, thereby resulting in massive layoffs and unemployment. The specific professions that have been identified include medical doctors, architects, engineers, journalists, lawyers and coders (Bolina, 2023). While it is true that ChatGPT can be trained to diagnose diseases based on patient's symptoms, detect problematic lesions that radiologists may miss in x-rays, churn out architectural plans, perform engineering calculations, write news stories, produce standard legal documents, and assist in writing codes, it does not necessarily follow that all the aforementioned jobs will be rendered redundant by AI.

Robert Solow, an economist who won the Nobel Prize in 1987 for his research on theories of economic growth, drew attention to the productivity paradox where he observed that the much-anticipated productivity growth often ascribed to computers failed to materialize. While multiple hypotheses have been advanced to explain for this paradox, including the “mismanagement of information and technology” (Brynjolfsson, 1993, p. 74), the paradox persists. Subsequently, Brynjolfsson et al. (2020) refined Solomon's thesis to derive their J-curve productivity hypotheses to describe initial slowdowns in productivity associated with the introduction of new technology that will be followed by a subsequent increase in productivity.

Hyman (2023) offered an even brighter scenario whereby ChatGPT can help free us from the repetitive aspects of our jobs to focus more attention to the human aspects of our work that cannot be performed by machines, thereby increasing creativity. This is so because person-to-person interactions and brainstorming sessions to generate truly innovative ideas cannot be replicated by AI as its capability is confined primarily to summarizing/analysing massive amounts of data and repackaging them in a generally intelligible format for consumption. As such, considering its potential benefits, instead of blanket rejection of ChatGPT we should embrace it for use in tasks that are repetitive and hence can be automated. In this way, organizations can channel employees' creativity to more challenging and complex activities that cannot be undertaken by sheer manipulation of large data bases. This redirection of focus will result in significant changes to HRM policies and practices. Companies that can successfully redesign, restructure, retool and reskill their workers to relieve employees from more mundane tasks to focus on creative and innovative endeavours will reap the benefits, thereby highlighting the significant role that such changes can play in a firm's competitiveness and a nation's global competitiveness.

In a report entitled, The Main Resource is the Human, Musser et al. (2023, p. 1), acknowledged that AI “is increasingly understood as a strategic technology that governments seek to promote domestically and constrain for adversaries”. Furthermore, the report went on to conclude that based on a survey of over 410 authors of papers in 20 top AI journals, the consensus is that “talent” is the most important contributor to the “success of their most significant

# BUDHWAR et al.

projects”, thus highlighting the pivotal role that the attraction, motivation and retention of talent can play in a nation's international competitiveness. Since talent is mobile, this contributes to the global war for talent, a stable subject of research in IHRM.

# 4.1.2 Diversity of perspectives

Research has shown that while diversity may lead to conflicts and disagreements in the shorter-term, it can contribute to increased creativity as different perspectives (stemming from a member's gender, race/ethnicity, background and so on) can be incorporated in deliberations to arrive at a final decision.

While ChatGPT has been trained on massive databases, the reality is the output reflects largely the algorithms that were written predominantly by white males. For one, AI can be racists and sexists as captured in the title of Noble's book, Algorithms of oppression: How search engines reinforce racism, Noble (2018). For example, in early 2018, Google's facial recognition function was found to be more accurate for identifying white men but less so for women, and even less so for African Americans. A report published by the US Department of Commerce's National Institute of Standards and Technology, prepared by Schwartz et al. (2022), highlights the disturbing incidence of biases against women, people with dark skin tones, older people, the disabled, and people wearing glasses. Algorithms with such built-in biases can result in discriminatory decisions pertaining to hiring/selection, promotion, firing, evaluation of performance, and compensation. In short, under the guise of science, the perpetrators of such biased decisions may defend the so-called objectivity of their actions.

Aside from possible biased decisions, it is important to recognize ChatGPT is trained on large datasets. Since most datasets are in English, they can disenfranchise people whose languages are very different from English. This includes East Asian languages, such as Chinese and Japanese as well as the variety of languages used on the African continent. The Chinese are, of course, working on their own ChatGPT-tools. Concerns have been raised that ChatGPT and its equivalents can “bring about a mass extinction of languages. And … more worryingly, would expunge a diversity of ways of thinking and creativity” since language is contextualized in a culture where it is used (Marian, 2023). In short, AI has the possibility of achieving what centuries of colonization have failed to accomplish, namely wiping out cultures and countries that are under-represented in the AI technological race. The impact of cultural diversity on theories, practices and policies of IHRM has been well documented and continues to receive research attention.

A research agenda under this theme includes, but is not limited to:

- Identification of biases inherent in algorithms that can discriminate based on select demographic variables and how to reduce/eliminate these biases in all aspects of HRM functions, policies and practices. Studies of this nature have practical and theoretical implications as they can contribute to our understanding of human-technology interactions.
- A more nuanced understanding of how AI can inhibit diversity, equity and inclusion in the functioning of multi-cultural teams (MCTs). Such findings can contribute to the theoretical literature on MCTs, a staple research topic in IHRM.

In summary, while I have identified only two broad themes that merit attention from the IHRM perspective, in reality, there are many, many more. As IHRM researchers devoted to the advancement of the field considering current events and most recent developments, we have an obligation to identify and better understand all the factors and aspects of our discipline that are impacted by ChatGPT to minimize the perilous consequences and harness the promises associated with this new technology.

# 4.2 | ChatGPT, EDI and implications for HRM

Fang Lee Cooke12 and Charmi Patel20

# 4.2.1 | Introduction

The growing use of ChatGPT by organisations in an expanding range of functions and activities, including HRM functions and activities (Howlett, 2023), has implications for equity, diversity and inclusion (EDI), especially in employment security, recruitment and selection, performance appraisal and pay. Previous research indicates that AI can be an efficient and effective way of managing HR functions to enhance fairness. However, ChatGPT is designed to synthesise information from the database available, applying algorithms used to train it. While ChatGPT is designed to be neutral, the data it deploys to generate the texts come from a wide range of sources from the Internet, which may contain biased opinions and prejudices including, for example, racist and sexist language. Therefore, the use of AI has mixed benefits and pitfalls for organisations adopting the technology and their employees.

# Employment security

A key function and benefit of ChatGPT is that it can search and generate basic documents on a specific topic rapidly such as producing job descriptions and person specifications in detail. However, these documents need to be checked and customised, which requires more advanced skills and knowledge. Indeed, the increased use of digital technology has led to the displacement of many workers at the lower occupational levels, including bank tellers, IT engineers and legal workers. For HR professionals, there may be fewer job vacancies at the lower level. Since women tend to be over-represented at the lower occupational levels, they may be disproportionally displaced. Equally, older workers have been the victims of technological changes due to their reduced capabilities to learn new technology. In the IT industry, workers above the age of 40 may be at serious risk of being made redundant.

# Recruitment and selection

An increasing number of organisations are using AI for recruitment and selection to increase efficiency and accuracy, especially when they must process a large number of applications. AI is also perceived to be more objective and thus enhances the fairness of the selection by avoiding human subjectivity and biases. Equally, it has been argued that AI is inherently a deductive process and neglects qualitative information which humans may be in a better position to pick up and interpret (Newman et al., 2020). A similar argument could be made for the use of Chat GPT in recruitment and selection. EDI is an area that would benefit from human inputs to detect nuanced and sensitive information from the applications.

# Performance appraisal

Performance appraisal is one of the most subjective functions of HRM and one that managers tend to dislike and avoid if possible (Keegan & Den Hartog, 2019; Posthuma et al., 2018). Where performance could be quantified, AI has been used for the appraisal of employees to inform HRM decisions such as reward, employee development and dismissal. AI is believed to provide neutral feedback without gender, race or other forms of bias. In cultural contexts where direct and face-to-face criticisms are avoided to preserve face for both parties (such as in Chinese societies), AI can potentially be a more effective medium for appraisals and help individual employees to improve their performance quietly due to reduced resistance and resentment because there is no human interaction and perceived bias. However, it has been argued that when AI is involved in the decision-making process, managers are more likely to act in a morally disengaged manner and justify their decision (Bandura, 1999). A case in point is Amazon which has been criticised for using AI to select poor-performing employees for dismissal (Lecher, 2019).

BUDHWAR et al. 27

Pay. Pay is another sensitive and contentious HR function in which perceived and actual fairness and equity is critical but difficult to achieve for institutional and organisational reasons. In fact, gender pay gap has been found persistent in many economies, developed and developing alike, and gender discrimination has been found one of the main reasons for gender pay gaps (Blau & Kahn, 2007; Lips, 2013). The use of ChatGPT may arguably entrench the gender pay gap as the data it draws from may be historical data with past bias against women, ethnic minorities and other socially disadvantaged groups, allowing these biases to be perpetuated and continue to be barriers to pay equity which undermines organisations' EDI efforts.

# 4.2.2 Research agenda for EDI

The use of ChatGPT and other OpenAI tools is still in its early stage, albeit rapidly developing in both creative ways of their adoption and regulatory interventions. There are ample research opportunities for HRM scholars. We outline a few of them here as examples to entice more attention for future research to advance the theorisation and management practice in this space.

1. Given the uneven quality of the data used by ChatGPT to generate documents, how can organisations ensure the quality of the materials it generates so that it does not undermine the principle of ethics and outcomes of EDI? Will the use of ChatGPT and other OpenAI tools influence managerial behaviour such that they behave differently in AI-human cooperation than in human-human cooperation?
2. How can ChatGPT and other OpenAI tools be used effectively to enhance fairness and procedural justice in HRM functions and activities through AI-human cooperation? What may be the antecedents and boundary conditions?
3. How are society-specific factors played out in the use of ChatGPT in different HRM functions and activities? For example, will the use of ChatGPT and other OpenAI tools help managers avoid giving critical comments to poor-performing employees and help employees gain face, thus maintaining a good work relationship?
4. How can ChatGPT help in content creation for job descriptions and competency guides while reducing unconscious/implicit biases and discrimination in recruitment/selection processes? How can organisations regulate the use of ChatGPT and similar AI tools to detect potential bias in the language and data fed to AI tools, and how can these be addressed to ensure transparency and fairness in recruitment/selection processes?
5. In what ways can ChatGPT and other OpenAI tools help with benchmarking information on salary ranges and benefits offerings? As salary transparency continues to take root, can ChatGPT help HR professionals reduce the gender pay gap and inequalities by providing a good reference point for objective and data-driven recommendations for competitive pay, rewards, incentives, and other benefit programmes?
6. How can ChatGPT be used to create a more inclusive onboarding experience for employees with disabilities, and what are the best practices for incorporating assistive technologies into the process? For instance, in what ways ChatGPT can be used to collect and analyse data on the experience of new employees during the onboarding process, and how can those insights be leveraged to improve ongoing diversity and inclusion efforts in organisations?
7. How can ChatGPT and other OpenAI tools be adopted to offer personalised training and development opportunities to employees based on their specific needs and interests (especially neurodiverse employees and those from underrepresented groups)? For example, how can ChatGPT help build lesson plans, learning objectives, and skills assessments from existing content while addressing gender, age, racial biases and more? Future research should explore the impact on employee learning and development and the effectiveness of AI in identifying relevant training programmes.
8. By leveraging natural language processing and machine learning algorithms, in what ways can ChatGPT help organisations identify areas where EDI efforts and initiatives are lacking?

# 9.

With the increasing popularity of remote work, in what ways can ChatGPT and other OpenAI tools be used to monitor employee productivity and well-being, as well as to offer support and resources? In what ways can HR departments use such OpenAI tools to create a more positive and productive remote work experience for employees while maintaining privacy and security?

# 10.

With the success of generative AI tools such as “Woebot”, in what ways can organisations/HR professionals leverage them to identify early signs of burnout or other mental health issues among employees and provide timely support and resources?

# 11.

What training and resources do organisations/HR professionals need to develop/design ethical infrastructures to facilitate AI-human cooperation to implement fair and trustworthy AI tools that promote EDI at work?

# 4.3

# Generative AI and sustainable HRM

Shuang Ren22 and Prasanta Kumar Dey15 Sustainable HRM concerns systems, practices and policies that ensure the supply of a sustained workforce while also considering the economic, social and environmental impact of these practices (Aust et al., 2020; Ren et al., 2023). As a relatively new concept, it emerges in response to a growing recognition of the need to align HRM practices with sustainability goals and values (Westerman et al., 2020). However, the inherent tension involved in integrating sustainability goals and values into HRM practices presents unique challenges for sustainable HRM to manage the needs of the organization, its employees, and the environment. As organizations continue to navigate the complexities and ambiguities associated with sustainable development, new technologies, such as generative AI, are introducing additional challenges and opportunities for sustainable HRM.

# 4.3.1

# Our point of departure: Closely involve generative AI in enabling HRM institutional entrepreneurship towards sustainability

A key challenge sustainable HRM faces in achieving widespread implementation is that it requires a new, multi-purpose model of organizational effectiveness that conflicts with the traditional market-oriented model (Ren et al., 2023; Stahl et al., 2020). HRM professionals, in their unique and important role as institutional entrepreneurs (Ren & Jackson, 2020), often struggle to find actionable guidance on how to promote institutional changes towards sustainability. Generative AI presents a promising tool to assist in this transformation process, offering potential solutions to address the challenges faced by HRM professionals.

First, generative AI can generate innovative ideas that are not yet considered by HRM professionals alone. For instance, it can stimulate different scenarios, solutions and outcomes to complex sustainability challenges, help organizations explore different sustainability strategies in a risk-free environment and make informed decisions on which ones to implement. Sustainable HRM often face resistance from line managers who see their roles as bottom-line oriented and marginalize other considerations. By generating scenarios that consider different levels of support from different management functions, generative AI can help organizations identify where additional support or engagement is needed to ensure the adoption of sustainable HRM.

Second, generative AI allows organizations to create personalized sustainability trainings or initiatives tailored to the specific needs of organizations' internal and external stakeholders. For instance, by gathering and analysing large amounts of data related to employee commuting routines and waste management habits, generative AI can suggest personalized initiatives to encourage the use of more sustainable modes of transportation. It can also provide real-time feedback on how job responsibilities can be undertaken in a more sustainable way or where sustainability-related skill gaps exist by analysing job role, training history and performance data. In turn these individualized initiatives strengthen the collective capacity of the organization and foster a culture of continuous learning related to sustainability, creating a more conducive institutional environment for sustainable HRM.

BUDHWAR et al. 29

Third, both sustainable HRM and generative AI play major roles as enablers to transform organisational supply chain to adopt sustainability practices to assure enhanced sustainability performance. The transformation can start with mapping supply chain, deriving current sustainability performance, identifying issues and challenges, setting targets, deriving enablers (including sustainable HRM practices and generative AI), analysing business case, and planning, implementing and evaluating improvement projects (Dey et al., 2022).

Fourth, generative AI, along with other technologies such as IoT, Industry 4.0, and Metaverse, has the potential to transform sustainable HRM practices by automating routine tasks, reducing bias in selection and performance evaluation, and improving efficiency. This can contribute to creating a more diverse, inclusive and sustainable workplace. Additionally, it can assist in analysing employee sustainability-related performance, engagement and satisfaction using large amounts of data, leading to data-driven improvement measures for overall business sustainability.

# 4.3.2 | Replacing or empowering a sustainable workforce?

A direct impact of generative AI on sustainable HRM is the disruption to future employment. The fact that technological advancements impact the job market is not a new phenomenon. In the past, technological progress has led to the displacement of workers in certain industries while simultaneously creating new industries and career fields. These industries are typically low-skilled that can be easily replaced by automation. However, generative AI, with its ability to create content and mimic human thought processes, has a unique advantage over traditional AI to impact a much broader range of jobs, including high-skilled ones. The encompassing disruption to future employment brings many unknowns for sustainable HRM in terms of how to support the long-term viability of the organization.

Relatedly, generative AI is likely to lead to an overhaul of how sustainable career is built (De Vos et al., 2020) and how the work-life boundary is defined in building human sustainability. To the extent it replaces jobs or professions that can be easily automated, it frees up employees from repetitive and routine tasks to focus on more complex and higher-level ones that require creativity, social intelligence and adaptability. Future jobs go beyond traditional occupations, and feature more inter-organization movements. Instead of continuing to have, for example, “lawyers”, “engineer”, and “doctors” in traditional terms, we may see new job roles such as “critical thinkers” who work in different organizations and industries simultaneously by applying their creative and innovative skills. The time freed up from handling routine tasks also enables employees to pursue a wider range of activities to achieve overall human sustainability by keeping healthy, productive, happy and employable throughout the life course. Hence it is not the replacement of jobs that is concerning, but how sustainable HRM can take advantage of the opportunities created by generative AI to help employees build new mindsets and skillsets for their sustainable careers.

# 4.3.3 | Known unknown versus unknown unknowns in adopting sustainable HRM practices

Research and practice on sustainable HRM have so far lack a tailored approach to design and implement practices that fit the needs of organizations. Generative AI provides the potential for organizations to better report their sustainability practices, adopt and monitor these practices across the organizations' value chain (Figure 5). Specifically, it facilitates developing the decision support system (DSS) that are essential to transform organisational value chain at strategic, planning, and operational levels through various sustainability practices (Dey et al., 2023). DSSs designed using generative AI will facilitate sustainable HRM capture data and select optimal options when undertaking both intra-organizational functions (e.g., manpower planning, training needs identification, fostering conducive organisational culture, enhancing employee wellbeing) and inter-organizational activities (e.g., promoting CSR initiatives in collaboration with the society). These are complex tasks that are likely be accomplished by generative AI on coordinated actions rather than individual function. In other words, generative AI presents evidence-based guidance.

# Sustainable HRM

# Inter-organization Sustainability practices across organizational value chains

# Sustainability practices

# Performance

# Design

# Economic Performance

Managing intra-organizational supply chain

|Recover|Resource efficiency|Procurement|
|---|---|---|
|Energy efficiency|Environmental Performance|Waste management|
|Corporate social responsibility activities|Renewable energy Consumption|Carbon offsetting|
|Production|Operations|Social Performance|
|Meeting compliance with policy regulations|Decision support system using generative AI for strategic, planning and operational level decisions across each function of organizational value chain|Decision support system using generative AI for strategic, planning and operational level decisions across each function of organizational value chain|

F I G U R E 5

# 4.3.4 Research agenda

Figure 6 highlights a few areas of research that are of particular importance. Within the organization, the intersection of generative AI and sustainable HRM raises several ethical issues not yet addressed by HRM scholars when facilitating structural, process, technology, and cultural changes towards sustainability. Specifically, algorithm needs to be carefully developed without bias. If the underlying assumptions exclude fairness, equality and privacy, they can lead to a biased and incomplete representation of sustainability. Even if the pre-training transformer is powered by a pro-green ideology, the narrow understanding of sustainability risks backlash from employees from a much diverse background. The lack of diversity may also risk reinforcing existing biases and stereotypes, which can lead to discriminatory or unfair AI models. Future research needs to devise ongoing evaluation and feedback mechanisms to detect and address any biases or ethical concerns that may arise. In addition, HRM has an urgent need to optimize formal data management policies in relation to the ethical collection, use and protection of personal and organizational data.

The energy consumption required for training and running large-scale generative AI models can be significant, raising the question of whether they are sustainable. Future research on sustainable HRM therefore needs to take a more critical stance in evaluating the broader role of technology in achieving sustainable development. Related research questions also include how to consider employee interpretation and attribution of the nature and efficacy of generative AI in contributing to long-term sustainability.

More broadly, future HRM research needs to contextualize the discussion of the implications of generative AI at the societal level, for example, by considering what we call “AI-precariats”, a social-economical group of labours disadvantaged by generative AI. This includes segments such as the ageing workforce or those with limited access or skillsets, who may face challenges such as skills obsolescence, job displacement, and further marginalization or exclusion from the workforce. Sustainable HRM needs to develop a more inclusive model of human sustainability in ensuring the viability of the workforce.

Introducing generative AI to sustainable HRM challenges HRM scholars to adopt a more collaborative stance to liaison with AI experts to ensure technology is used effectively and ethically to support sustainable practices. Insofar as AI technology helps to put sustainable HRM into action, sustainable HRM also provides feedback loop to AI experts in terms of how generative AI can be developed and used in a way to minimize negative environmental impacts, promote economic and social equity, and contribute to long-term sustainability. Future research should identify the needed skillsets for HR professionals to engage with generative AI in the adoption and evaluation of sustainable practices.

An international or comparative lens is also needed to delve deeper into the sustainable HRM-generative AI nexus across various institutional contexts. For instance, Goldman Sachs (2023) suggests that Hong Kong, Israel, Japan, Sweden and the US are likely to be the most affected countries, while India, Indonesia, Pakistan, Philippines and Vietnam are the least affected. More research is needed to understand how and why; otherwise, income inequality and social polarization, if not accompanied appropriately with policies and governance for generative AI, will undermine sustainability efforts.

# BUDHWAR et al.

|Organisation|Processes|Technology|Culture|
|---|---|---|---|
|Socio-economic inclusion (e.g AI precariat)|Stakeholder management|Generative AI to develop DSSs to transform organizations towards sustainability (structure, processes, technology and culture)|Generative AI-ready skillsets|
|sustainable HRM using an international perspective|sustainable HRM using an international perspective|sustainable HRM using an international perspective|sustainable HRM using an international perspective|
|Inter-organizational collaboration|Inter-organizational collaboration|Inter-organizational collaboration|Inter-organizational collaboration|

Research scope of generative AI to embed sustainability within organizations through sustainable HRM.

F I G U R E 6

# 5 | GENERATIVE AI AND ACADEMIC RESEARCH METHODS

# 5.1 | Insights and recommendations from a research methods' conversation with ChatGPT

Mark NK Saunders23

# 5.1.1 | Introduction

Transformative artificial intelligence tools, including LLMs such as ChatGPT offer new opportunities to generate sophisticated text. However, although this may, at least initially, appear convincing, it is accompanied by potential for negative impacts (Dwivedi et al., 2023). In this piece I reveal that using such tools uncritically is irresponsible and a derogation of the researcher's duty to undertake rigorous and ethical research. In other words, it remains the researcher's responsibility to acknowledge generated text and ensure it stands up to the closest scrutiny. Drawing Mingers (2000) work on adopting a critical mindset and Habermas's (1984) validity claims, I assess ChatGPT generated text responses relating to participants and sample size justification. The implications of these are discussed and recommendations for asking questions and evaluating LLM generated text offered.

# Adopting a critical mindset

As researchers we use of a wide variety of tools such as online databases and data analysis software; but we do so with a critical mindset. We are trained, for example, to be cognoscente of the need to evaluate the validity and utility of data and to ensure the analysis techniques used are appropriate for our data. Similarly, when we read text, we need to do so with a questioning and sceptical mindset, adopting a critical approach. Drawing on Habermas's (1984, 1992) theory of the validity claims of speech acts, Mingers (2000, p. 225) identifies four dimensions to such an approach:

- ‘Critical thinking -the critique of rhetoric’ questioning the logic of argument and fair use of language.
- ‘Being sceptical of conventional wisdom -the critique of tradition’ questioning taken-for-granted assumptions about traditional ways of doing things.
- ‘Being sceptical of one dominant view -the critique of authority’ questioning the dominant position and recognising a plurality of viewpoints.
- ‘Being sceptical of information and knowledge -the critique of objectivity’, questioning the validity of knowledge and information recognising knowledge is never value free and objective.

These, in conjunction with Habermas's (1984) validity claims of non-deceptiveness (sincere), factual correctness (truth), and normative acceptability (rightness), are used to assess text generated by ChatGPT.

# 5.1.2 | An assessment of ChatGPT generated responses

My initial question to ChatGPT: “Can you write a methods section to justify the number of interview participants in a workplace study?” evoked five separate paragraphs comprising ‘participants’, ‘interview conduct’, ‘data analysis’, ‘justification for sample size’, and ‘conclusion’; but without any references. I therefore asked: “Can you add academic references to the text?” The subsequent critique begins by considering the paragraph relating to ‘participants.

Participants: Participants for this study were selected from a large manufacturing organization in the United States. A total of 20 employees were interviewed for this study. Participants were selected based on their experience with the organization, length of service, and job role. (ChatGPT, 2023)

# BUDHWAR et al.

ChatGPT created a study in a large US manufacturing organisation from which 20 employees were interviewed. Although it is unclear how or why this setting was chosen, it provides a useful context within which to evaluate the subsequent justification for sample size. In relation to the ‘participants…’ paragraph, I developed the chat, asking “How and why were participants were selected based on their experience…” listing all characteristics named in ChatGPT's previous response. The answer focussed on why, the rhetoric relating the justification appropriately to a now stated research purpose “to gain insights into workplace culture and practices”, and that the “range of experiences and perspectives” held by participants with different “lengths of service and job roles” would “allow for a deep [er] understanding of the organizations' culture and practices”. When asked to “explain what sample selection technique might be used and give references?” purposive, snowball, convenience and maximum variation sampling were suggested. This generic response, whilst acceptable in terms of factual correctness, offered no convincing arguments regarding their potential advantages and disadvantages to support any critique of tradition.

This justification for sample size paragraph appears, initially, comprehensible, offering a logical argument and using language fairly. However, applying the critique of objectivity and confirming the validity of the text raised concerns. Generated text infering Guest et al.’s (2006) explored a similar research question in a workplace setting was neither related to the context nor true. This highly cited Field Methods paper on ‘how many interviews are enough’ was based upon data collected originally to explore “perceptions of social desirability bias (SDB) and accuracy of self-reported behaviour” in the context of reproductive health research, the sample comprising 60 female sex workers in Nigeria and Ghana (Guest et al., 2006, p. 62).

# Justification for Sample Size

The sample size of 20 participants was deemed sufficient for this study based on several factors. Firstly, the population from which the participants were selected was relatively small, with only a few hundred employees in total. Secondly, the sample size was consistent with previous studies that have explored similar research questions in workplace settings (Guest et al., 2006). Additionally, thematic saturation was achieved with the sample size of 20, meaning that new themes were no longer emerging from the data after the 20th interview (Francis et al., 2010; Morse, 1995). (ChatGPT, 2023)

The other two references (both high cited) used to justify thematic saturation comprised an editorial in Qualitative Health Research (Morse, 1995) and an empirical study in Psychology and Health (Francis et al., 2010). Given neither of these references were from mainstream management journals, I asked: “Can you add management references not in the text?” ChatGPT responded extending the justification paragraph:

In management literature, Eisenhardt (1989) and Glaser and Strauss (1967) suggest that a sample size of 20–30 is appropriate for building theories from case study research and grounded theory, which are common approaches to analysing qualitative data in management research. Moreover, Hennink et al. (2017) argue that the determination of the sample size in qualitative research is not necessarily about the number of participants but rather about reaching data saturation, which occurs when no new information or themes are emerging from the data. (ChatGPT, 2023)

On initial reading this extension appears to satisfy both critiques of rhetoric and objectivity. Saunders and Townsend (2016), in their consideration of what editors and reviewers have deemed acceptable, note that 30 participants when analysing data for single groups such as an organisation are likely to be credible. However, in referring to theoretical sampling, this rhetoric contradicts the sampling techniques offered in response to an earlier question. Critiquing the rhetoric more closely reveals problems with the premises of the argument and the validity (truth) of the knowledge as reported. The generated text notes correctly that Eisenhardt's (1989) paper is about building theory from case studies. However, her discussion relates to the number of cases (not participants) required to

BUDHWAR et al. 35

reach closure, noting “while there is no ideal number of cases, a number between 4 and 10 cases usually works well” (Eisenhardt, 1989, p. 545). Similarly, whilst noting Glaser and Strauss' (1967) book is about grounded theory, the generated text does not report their actual advice: “the adequate theoretical sample is judged on the basis of how widely and diversely the analyst chose his groups for saturating categories according to the type of theory he wished to develop” (Glaser & Strauss, 1967, p. 63).

More broadly it is clear that other recent research that could be pertinent was not included in the generated text; for example, Mthuli et al. (2022) articulation of considerations in qualitative research sample size determination. Perhaps being somewhat narcissistic, I asked ChatGPT to address this by “adding in” a reference to my own research on sampling in organization and workplace research -Saunders and Townsend (2016). This it did inserting it after “Guest et al., 2006” in the ‘Justification…’ paragraph: However, the full reference provided was untrue: “Saunders, & Townsend. (2016).” Using grounded theory in qualitative research: Theoretical and practical issues. In The Sage handbook of qualitative research in psychology (pp. 27–43). Sage publications (ChatGPT, 2023). Whilst this handbook has been published (Willig & Rogers, 2017), the chapter on grounded theory has an alternate title and different co-authors.

# 5.1.3 Discussion

ChatGPT appears initially to be good at sorting, reproducing and summarising information in a style convincing to a native English speaker. Yet, my consideration reveals this can be illusory and, crucially, LLM generated text needs to be approached with a critical mindset. Recognising this, I offer six recommendations for asking questions and evaluating LLM generated text as part of this discussion.

Scepticism of conventional wisdom and taken-for-granted assumptions was absent in the ChatGPT text, although alternatives (such as different sample selection procedures) were offered. The text generated offered the dominant view, for example, sample size for interviews rather than recognising a plurality of viewpoints. This absence of both a critique of tradition and of authority may be due, at least in part, to my inexperience regarding prompting ChatGPT with questions which generate more relevant answers (Myklebust, 2023). Given this, my first and second recommendations relate to questioning LLMs:

1. Learn how to asks effective questions to answer your queries.
2. Use questions actively to ask for alternate views.

Rhetoric of answers provided by ChatGPT on the surface, offered a semblance of a logical argument appearing plausible. Yet, on closer examination flaws were apparent in the logic of argument both within and between paragraphs. Hence my third recommendation:

Check the logic of the argument consistent both within and between paragraphs.

Confusingly, ChatGPT generated differing answers when asked the same question two or more times, potentially suggesting deception. Misrepresenting research reported in both a cited book and article and providing an incorrect reference for another article highlighted issues with truth. While ChatGPT (2023) explicitly warns it “may occasionally generate incorrect information” and “it is important to verify information from multiple sources including academic journals” these, along with a lack of transparency regarding closed-access sources consulted, are concerning. ChatGPT's (2023) apparent exclusion of recent research and stated “limited knowledge of the world and events after 2021” is another limitation. My fourth and fifth recommendations when using LLM generated text are therefore:

Establish the trustworthiness of the information provided and their associated sources by confirming the actual academic sources wherever possible, noting that some sources may not actually exist.
Recognise that the information provided is bounded in terms of dates and what can be accessed and use alternative valid and sources.

# 36

# BUDHWAR et al.

In conclusion, LLMs as artificial intelligence tools cannot take responsibility for the content that is generated (Myklebust, 2023). Rather, accountability for work generated by LLMs such as ChatGPT should therefore remain with the authors (Nature, 2023). This has implications for reporting LLM generated text and its use, hence my final recommendation:

# 6. Be transparent in the in acknowledging and explaining the use of the generated text, treating the generator as a tool rather than co-author.

# 5.2 Using generative AI as a methodology assistant: Trust, but verify

Herman Aguinis7 and Jose R. Beltran9

We offer a critical evaluation of the extent to which generative AI, such as ChatGPT can serve the role of a methodology assistant for research in human resource management (HRM) and other domains. Specifically, we address how ChatGPT can be a valuable tool in assisting researchers at each of the typical stages of an empirical research study: (1) theory and idea generation, (2) design and ethics, (3) measurement, (4) analysis, and (5) reporting of results. We conclude that there is potential in using generative AI as a useful methodology assistant; however, we also show that researchers can trust some of the advice of ChatGPT but must always verify its recommendations.

Next, we describe its capabilities as well as limitations and illustrate them by asking ChatGPT to provide advice about each of the stages of research based on the excellent study by Purcell and Hutchinson (2007), which is one of the most impactful Human Resource Management Journal articles published in the past few years (i.e., about 1500 Google Scholar citations). Specifically, we used ChatGPT as a methodology assistant in conducting a study like Purcell and Hutchinson. As a preview and summary of our results, Table 1 includes the questions for which we asked ChatGPT's assistance and the recommendations we received.

# 5.2.1 Generative AI as a methodology assistant: Capabilities

During the initial stage of theory and idea generation, ChatGPT can support researchers in several ways. This includes generating ideas based on research questions or topics of interest and proposing relevant theories and concepts that could be beneficial in addressing the research questions. In our case study based on Purcell and Hutchinson (2007), Table 1 shows that ChatGPT described several possible theoretical frameworks, such as the resource-based view, contingency perspective, and social exchange theory.

In the design and ethics stage, ChatGPT can also be helpful by guiding researchers in choosing suitable data collection approaches that comply with ethical standards. In our case study, ChatGPT suggested a cross-sectional survey design and identified specific constructs on which to collect data, as well as the different levels of analysis at which these constructs should be assessed. Regarding ethical issues, ChatGPT provided some broad principles (e.g., informed consent, anonymity, and confidentiality, data protection).

At the measurement stage, ChatGPT can assist with selecting specific instruments. In the context of our case study, ChatGPT recommended using several scales for measuring job commitment and leadership quality: Organizational Commitment Questionnaire, Affective Commitment Scale, and Multifactor Leadership Questionnaire.

Concerning the analysis stage, ChatGPT cannot yet conduct analyses (Burger et al., 2023). Still, it can support researchers by helping them choose appropriate statistical techniques or generate code to run specific analyses for software such as R, Python, SPSS, and SAS. Moreover, it can help regarding critical data-analytic decisions such as managing outliers and missing data and which control variables to include in the analysis. In our case study, ChatGPT's suggestion to use multiple regression analysis is consistent with the approach taken by Purcell and Hutchinson (2007).

# BUDHWAR et al.

# 37

# T A B L E 1

# Using Generative AI as a Methodology Assistant: Case Study on using ChatGPT for Examining the Role of Front-line Managers on the Link between HRM and Organizational Performance (cf. Purcell & Hutchinson, 2007).

|Questions|ChatGPT responses|
|---|---|
|Stage 1: Theory and idea generation|The question uses the resource-based view, contingency perspective, and social exchange theory to emphasize the importance of effective HRM practices in achieving organizational performance and competitive advantage. It also emphasizes the need to tailor HRM practices to suit the needs of the team and the organization and highlights the importance of front-line managers' interpersonal and communication skills in managing employees effectively.|
|Stage 2: Design and ethics|A cross-sectional survey research design could be used to test whether employee commitment towards their employer and job is influenced by the quality of leadership behaviour and satisfaction with HRM practices. The study can collect data from employees in different departments and levels of the organization through survey questionnaires. The questionnaires can include questions that measure employee commitment towards their employer and job, leadership behaviour quality, and satisfaction with HR practices.|

(Continues)

# BUDHWAR et al.

# T A B L E 1

# (Continued)

|Questions|ChatGPT responses|
|---|---|
|With the suggested research design, what steps should be taken to ensure it complies with all ethical standards to determine whether employee commitment towards their employer and job is influenced by the quality of leadership behaviour and by satisfaction with HRM practices?|1. Informed consent: Obtain informed consent from the participants before collecting any data. Participants must be provided with clear and understandable information about the purpose, procedures, and potential risks and benefits of the study, and they must be given the opportunity to ask questions and withdraw from the study at any time without penalty. 2. Anonymity and confidentiality: Ensure anonymity and confidentiality of the data by not collecting any identifiable information such as names, addresses, or other personal information. Participants will be informed that their responses will be kept confidential, and that the data will be used for research purposes only. 3. Protection of vulnerable populations: Ensure that vulnerable populations such as minors, individuals with disabilities, and individuals with limited literacy or language skills are protected by providing additional safeguards and informed consent procedures. 4. Data protection: Ensure that the data is protected from unauthorized access or disclosure by implementing appropriate data security measures such as encryption and password protection.|
|Stage 3: Measurement What measures should be used in the study based on the suggested research design?|To measure employee commitment, a validated scale such as the organizational commitment questionnaire (OCQ) or the affective commitment scale (ACS) could be used. These scales ask employees to rate their emotional attachment and loyalty to their organization. To measure the quality of leadership behaviour, a leadership behaviour questionnaire such as the multifactor leadership questionnaire (MLQ) could be used. This questionnaire measures leadership behaviour in terms of transformational, transactional, and laissez-faire leadership styles.|

# BUDHWAR et al.

# 39

# T A B L E 1

# (Continued)

|Questions|ChatGPT responses|
|---|---|
|Stage 4: Analysis|Assuming the suggested research design and measures are used, what are possible ways to analyse the resulting data? To analyse the data, multiple regression analysis could be used. The dependent variable would be employee commitment, while the independent variables would be quality of leadership behaviour and satisfaction with HR practices. Control variables such as employee demographics, job characteristics, and organizational factors would also be included in the analysis.|
|Stage 5: Reporting of results|What best practices for reporting results of the previous analyses should be included regarding transparency? 1. Clearly state the research questions and hypotheses: This will help readers understand the purpose of the study and the specific relationships being examined. 2. Describe the sample: It is important to provide information about the sample size, demographics, and any other relevant characteristics that may affect the generalizability of the findings. 3. Report the measures used: This includes reporting the specific scales and items used to measure each construct, as well as information about their reliability and validity. 4. Present the results in a clear and understandable manner: This could include tables, charts, or graphs that summarize the findings in a way that is easy to interpret. 5. Report effect sizes: Reporting effect sizes, such as correlation coefficients or beta coefficients, can provide readers with information about the strength and direction of the relationships between the variables. 6. Report statistical significance: If statistical significance tests were conducted, it is important to report the p-values and the significance level used, so readers can understand the probability that the findings are due to chance. 7. Discuss the limitations of the study: It is important to discuss any limitations of the study that may affect the validity and generalizability of the findings.|

# 5.2.2 Generative AI as a methodology assistant: Limitations

ChatGPT's critical limitation is its reliance on constrained training data resulting in “feast or famine” results: too many, too few, or too general. As of the writing of this article, ChatGPT relies on training data before 2022, includes limited access to academic articles, and is unable to collect new data. So, ChatGPT is beholden to the past and can only recommend doing things as they were before. If methods were used incorrectly, ChatGPT will nevertheless recommend repeating what has been done already making it particularly vulnerable to methodological myths and urban legends (Lance, 2011).

Our case study illustrated several limitations of using ChatGPT as a methodology assistant, including too many suggestions, too few suggestions, and too general suggestions. First, for some questions, ChatGPT provided an abundance of responses (e.g., more than one measure for the same construct) that could lead to decision paralysis for junior researchers who may not know which response to choose or may be overwhelmed by the number of responses provided. Second, while ChatGPT has made significant advancements in natural language processing, it still has a limited contextual understanding of problems (Farndale et al., 2023) that could potentially limit the number of returned suggestions. For example, in the analysis stage, ChatGPT recommended multiple regression. However, multilevel modelling is more consistent with the measurement recommendation to collect data for constructs residing at different levels of analysis. Finally, regarding the suggestions about reporting, while ChatGPT's list of seven recommendations was comprehensive, they did not include enough specificity to apply to the particular context of our study fully. Similarly, suggestions about conducting ethical research were based on general and broad principles rather than specific and actionable recommendations, given our study's specific characteristics and context.

# 5.2.3 Research directions and considerations of ethical research

Our critical analysis of the capabilities and limitations of using generative AI as a methodology assistant led to several future research directions. First, our case study did not address qualitative research in particular. Thus, research is needed to understand generative AI's capabilities to assist qualitative researchers with, for example, transparency and replicability challenges (Aguinis & Solarino, 2019). Second, our analysis is based on ChatGPT's capabilities as of the writing of our article. We anticipate that future improvements will require additional evaluations. Third, future research can examine generative AI's capabilities and limitations when providing specific recommendations for statistical analysis for more specific contexts and situations. For example, to what extent does it offer useful and accurate suggestions on which specific assumptions are tenable and which specific models (e.g., fixed vs. random effects) should be implemented? Our case study is just the beginning of a research agenda aimed at understanding generative AI's capabilities and limitations as a methodology assistant.

Finally, using generative AI as a methodology assistant raises unique ethical considerations. First, publishers and journals are now reaching a consensus on how AI's writing assistance should be acknowledged in submitted manuscripts (Eddleston et al., 2023; Elsevier, 2023). However, can generative AI help researchers address possible ethical dilemmas and trade-offs? What choices would ChatGPT make when asked about solving ethical dilemmas? Would it embrace a value-based (i.e., deontological) or cost-benefit (i.e., utilitarian) philosophical approach to ethics? Can it anticipate possible research misconduct and help researchers prevent a future retraction? There is a need to address these and many other related questions.

# 5.2.4 Conclusions

We conducted a case study to critically evaluate the potential of generative AI as a methodology assistant. ChatGPT has useful capabilities: it can help researchers generate ideas; suggest theories; select research designs, measures, and data-analytic approaches; and provide general guidance on how to conduct ethical research and report results. However, ChatGPT also has limitations related to constrained training data, which result in generating too many, too few, or too vague recommendations, including repeating methodological errors of the past and offering recommendations on research ethics that are too broad and not sufficiently actionable or specific. Thus, we conclude that generative AI can be used as a complementary tool but not as a replacement for researcher judgement. Using a phrase made famous by former U.S. President Ronald Reagan, when using generative AI as a methodology assistant, our recommendation is to trust, but verify.

# 6 AI, ECONOMY, SOCIETY AND HRM: POSSIBILITY, UNCERTAINTY AND RISK

Pawan Budhwar1, Soumyadeb Chowdhury2, and Geoffrey Wood3,4,5,6

The rapid advances in AI technologies have opened new possibilities for HRM practices and research. Generative AI has the potential to revolutionize the way organizations manage their workforce, from recruitment and selection to employee engagement and performance management. However, while the use of generative AI in HRM has the potential to increase efficiency, accuracy, and effectiveness, it also raises important ethical, moral, legal, and social implications that must be carefully examined. As with any new technology, the full consequences will only become apparent some time down the line. A further indeterminacy is around its present state of development; it remains unclear as to how much it exhibits is genuine and autonomous intelligence and how much is simply a mechanistic implementation of complex sets of algorithms. In this research agenda setting section, we outline a series of research themes importing common themes from the perspectives presented so far. The aim is to deepen our understanding of the opportunities and challenges of using generative AI in HRM. Specifically, we aim to explore the potential of generative AI for improving HRM practices, while also addressing issues related to bias, discrimination, privacy, and transparency.

# 6.1 Theme 1: Work, skills and education

Generative AI applications have the potential to change the meaningfulness and nature of work, which may lead to direct job displacement within the HRM sector and may challenge human creativity. This is particularly so given that AI presently aims to solve problems through recourse to the knowledge it has access to; this may lead to the space for independent “blue sky” strategic thinking to be diminished. AI may also lead to a significant slimming down of the process side of the HR job, supplementing the earlier impact of HR software; the latter led to much basic HR administration being partially automated and partially delegated to line management. The existing literature (Daugherty et al., 2019; Wilson et al., 2017) suggests the new categories of jobs AI will create that will require employees to train AI systems (trainers), understand, interpret, communicate and assess the relevance of outputs (explainers), and manage regulations and policies governing AI systems (sustainers). This, of course, assumes that if genuinely intelligent, AI systems may wish to be trained. If their intelligence is overstated, this may require complex tinkering with algorithms that may require very specialist technical knowledge, but not necessary understanding, creating further issues.

In the City of London in the run-up to the 2008 financial crisis, a great deal of damage was caused by those capable of devising complex mathematical trading models, but who lacked a basic understanding of finance. It is quite probable the same risks could be generated by those engaged in building, training, or governing AI HRM systems; knowing about AI system design does not make for an understanding of HRM. Some tasks that were previously done by humans, such as data entry and analysis, may now be automated. This could free up time for employees to focus on more strategic and creative tasks, which could increase the meaningfulness of their work. However, it could also lead to a sense of disengagement or lack of purpose for employees who are now doing more routine tasks.

In practical terms, AI may relieve a great deal of the drudgery around, say payroll, and help drive more equitable reward systems; it may also add to the overall administrative burden by generating ever more petty decisions for approval. According to a recent report by Goldman Sachs, AI could replace the equivalent of 300 million full-time jobs (Vallance, 2023), creating ‘AI anxiety’ among workers about prospect of their role being superseded by technology (Cox, 2023; PWC., 2022). It might be anticipated that individuals with robust digital competencies, along with skills such as creativity and teamwork, which are more challenging for machines to mimic, will benefit from an AI revolution. It may also lead to the creation of more ‘bullshit jobs, that is ‘… a form of paid employment that is so completely pointless, unnecessary or pernicious that even the employee cannot justify its existence’ (Graeber, 2018, p. 9), in this instance carrying out basic bureaucratic work and petty decision making necessitated by all the new information of variable quality that might be generated through AI. Future research studies in this area could examine the antecedents influencing generative AI adoption among employees and cross-functional teams within organisations. It is also important to understand the new categories of jobs and tasks within HRM, and how existing roles and responsibilities will be either displaced or become completely redundant, and how AI may in fact make new and potentially meaningless work through supplying a plethora of marginally useful information and points for action.

The use of generative AI in HRM may require new skills from HR professionals, as well as other employees. For example, employees may need to learn how to use new generative AI tools for training, development, and performance management. HR professionals may need to develop new skills related to data analysis, algorithmic decision-making, and ethical considerations. HRM programmes may need to incorporate new content related to the use of generative AI, including topics such as data analytics, algorithmic decision-making, and ethical considerations. HRM students may also need to develop skills related to using generative AI-based tools and systems and be aware of the potential limitations related to their use. One of the major issues of AI is that a lack of information often results in information being concocted. This may be due to inherent intelligence or simply because it is built into the algorithm. If the latter, inherent in the design of AI is the deliberate generation of wrong information; this raises the risks of decisions being made based on falsehoods.

# 6.2 Theme 2: Business productivity

Past reports have shown that organisations face myriad challenges to integrate AI systems within their existing business processes, which often do not lead to promised business gains (Deloitte, 2017; The Economist, 2020). We expect organisations to face similar issues with generative AI tools. While we are yet to solve the problems related AI integration, businesses are likely to be cautious on adopting GAI due to six reasons: (1) huge capital investment and uncertainty on returns; (2) data infrastructure and volume of training data required to power GAI applications; (3) human resource skills, capabilities and mindset needed to collaboratively work with GAI assistants; (4) reliability of generative AI recommendations to redesign and reconfigure existing business practices; (5) potential security breaches; and (6) (perhaps most importantly) the unknown consequences that the usage of AI may hold. While untested claims have been made on the potential positive impact of generative AI tools on business productivity, we should learn from some of the recent developments in the emerging technology sphere. For example, blockchain was hugely overhyped; the platform TradeLens, provided by IBM and used by Maersk to optimise logistics processes involved in global trade, was discontinued due to lack of adoption by supply chain partners (Maersk Press Release, 2022). Moreover, proof of work blockchain is energy intensive – and inefficient – and has an enormous carbon footprint. Proof of stake blockchain is in turn, dependent on the crypto stakes being tendered being seen as worth anything. Similarly, the hype created by metaverse was also short-lived as Meta, Disney, Snapchat and Microsoft closed their division and abandoned existing projects (Quiroz-Gutierrez, 2023). While generative AI applications have the potential to become a gamechanger in certain sectors such as digital marketing and communications, information management, and education, it is still unclear how this new family of AI can create productivity at both organisational and employee levels. Managers need to strategically orchestrate resources to turn generative AI capability into valuable and resilient business outcomes. However, limited knowledge and understanding on how to achieve productivity using generative AI warrants further investigation.

BUDHWAR et al. 43

There is a difficult trade-off between on the one hand, adopting a major technological advance too late, and losing out to competitors, and, on the other hand, falling for hype and adopting a largely untried “solution” that may consume vastly more resources than its capacity to create value. Here it is worth noting that a body of opinion holds that, to date, AI does not exhibit the characteristics of autonomous intelligence at all (Broussard, 2018; Schweizer, 2022). Why this matter is that we do not know what has been built into the design of AI systems. For example, a capacity to falsify or lie may simply be a mechanism designed to provide speedy solutions through shortcuts; this raises serious questions about the ethics of designers and whether obvious untrustworthiness in one area is bound to be followed by still greater untrustworthiness in others.

# 6.3 | Theme 3: Sustainable HRM and productivity

The existing research has reported that adoption of industry 4.0 (I4.0) technologies such as cloud computing, AI, internet-of-things, and additive manufacturing can influence circular economy business models within organisations to achieve sustainable business performance (Dey et al., 2023; Luthra et al., 2020). Irrespective of these developments, we are no-where close to achieving net zero goals, as businesses, especially small and medium-sized enterprises struggle to find synergies between digitalisation, purpose driven, socially responsible and environmentally friendly practices. Recent literature shows the significance of green HRM practices are critical to green knowledge creation within firms which will drive green operational performance (Wang et al., 2023). In this context, the role of green awareness, skills and expertise is critical to drive sustainability agenda within businesses. The role of generative AI to implement, promote and manage sustainable HRM is fuzzy and less discussed in the current literature. For instance, generative AI applications can provide personalised training to employees catered to their distinct roles and responsibilities and cross-functional teams, which may drive motivation and attention. Similarly, generative AI as conversational bots may provide gamified experiences to employees, which may increase green engagement. Therefore, future studies should focus on how generative AI can increase sustainability awareness within organisations.

However, the unique features that make generative AI more powerful than its predecessors also consume more energy resources, which will lead to a heavier toll on the environment (Saul & Bass, 2023). Therefore, future studies should examine how to optimise the use of generative AI to achieve both resources and energy efficiency within HRM processes. Empirical studies should report frameworks and guidance for HRM managers that will help them to develop strategies for achieving digital sustainability through green training, empowerment, socialisation and involvement. These frameworks should identify resources, capabilities and knowledge required within organisations to implement generative AI systems conducive to the strategic goals of the organisations, adhering to the net zero policies and contributing to the welfare of the employees. Once more, there are risks; the causes of Internet outages are sometimes obscure, and a general Internet outage cannot be ruled out (Forscey et al., 2022). Given the lack of knowledge on how AI systems really work, the risks around systemic failure in the latter are much greater, in addition to obvious confidentiality and hacking concerns.

# 6.4 | Theme 4: Bias, ethical and moral judgements

Traditionally, output responses produced by complex AI algorithms are unexplainable, which makes the reliability and validity of these outputs untrustworthy (Chowdhury et al., 2022). These algorithms can also amplify bias in decision-making, which stems from the quality and nature of the data used to train it, algorithm design, and management of the automated process (Kelan, 2023). Generative AI presents various reputational risks for organisations, such as opaque algorithms that are hard to understand or audit, unfair treatment or prejudices based on demographics or other factors, use of inappropriate language or content, unauthorized use of copyrighted material or copied text, creation of false or misleading information, and manipulation of images or videos (Dilmegani, 2023). It can potentially violate people's privacy by generating highly detailed images or texts that reveal personal information or

preferences. It can create content that is difficult to attribute to a specific author or source, making it hard to deter-

mine accountability or trace the origin of the information (Appel et al., 2023). While issues with transparency and bias remain unsolved, recently European Union lawmakers have proposed full disclosure of any copyrighted materials used by companies deploying generative AI (Reuters, 2023). The adoption of generative AI within business organisa-

tions will depend on ability to mitigate risks and threats, achieve cyber resilience and embedding privacy preserving mechanisms. Therefore, future research should explore how risk management and cyber security strategies can lead to digital ethics, which will promote digital responsibility. In this context we have a responsibility to deepen our understanding of how to promote positive outcomes of the ongoing digitalization powered by generative AI systems, while also protecting people from potential negative effects. This requires us to consider the notion of digital respon-

sibility and how it applies to our work in varying contexts.

Recently, creators of ChatGPT attempted to explain the underlying reasons for the outputs produced by a GPT2 model using GPT4, which will help users to better understand the behaviour of the model (Griffin, 2023). However, the explanations were poor, which was attributed to AI unable to explain the complex behaviour of these models in a language that humans can understand. For instance, Facebook uses human content moderators, to remove inappro-

priate, disrespectful, or offencive posts because AI algorithms are unable to detect and remove content that does not match the community standards (Meta Help Center, 2023). Therefore, human actors will play and key role to review AI outputs and facilitate developing safety mechanisms that will drive ethical and responsible implementation and evolution of generative AI applications within businesses.

The explainability issues in future generative AI models such as ImageBind (Vincent, 2023), which combines multiple data streams such as text, audio, visual data, temperature, depth information and physical movement read-

ings to generate responses, may inhibit their adoption and commercial applications. While, this model will be made open source by Meta, there is also lack of consensus among the technology developers on open sourcing. On the one hand, advocates of open sourcing believe that it allows experts to scrutinize models that may help mitigate reli-

ability issues and evolve them for societal good, creators of GPT4 (Open AI) are wary of models being copied by rival competitors which will reduce competitive advantage, and being misused by bad actors.

Although we understand the risks and issues associated with generative AI models, HRM scholars can contrib-

ute to ‘interpretability research and open sourcing’ by understanding the role of different stakeholders within and between organisations in the business ecosystem that will help promote ethical use of such applications. In this vein, HRM research should develop actionable insights drawn from empirical evidence and case-studies on the best prac-

tices to train generative AI models, explain how decisions are made by humans when automated opaque algorithms are used to analyse data, embed robust mechanisms to increase safety and security of such systems, assess and mitigate the risks, and help the organisations achieve and maintain a desired level of quality.

# 6.5

# Theme 5: Research methods

The impact of generative AI tools on the way we develop, and conduct research has emerged as one of the most popular themes in the literature across disciplines. The ability of tools such as elicit to process, analyse and summarise research articles has fuelled heated debates about the future of academic research, especially future of review articles (AI Research Assistant, 2023). Moreover, acknowledging AI bots has authors in research articles (e.g., Agarwal et al., 2022) has put complex unanswered questions about research ethics and integrity (Dasborough, 2023). Generative AI tools can inte-

grate different literature to generate a very descriptive review based on the research papers uploaded by a researcher. However, we are yet to come across a unique contribution critically analysing strengths and weaknesses of multiple viewpoints, describing inherent meaning behind the viewpoints, and finally leading to novel theoretical developments done by generative AI. For instance, bibliometrics analysis has become a popular method to understand research trends by analysing bibliographic data obtained from research databases using software tools (Mukerjee etal., 2022). However, the analysis does not automatically and without expertise and knowledge of human researchers, lead to critical insights,

BUDHWAR et al. 45

unearthing reason behind interesting phenomena and evolution of theoretical contributions and such is the case with generative AI. Irrespective of the preliminary guidelines and policies provided by the publishers on the use of AI tools in research (for e.g., Elsevier, 2023), the proliferation of such tools will increase and ought to become more sophisticated. Therefore, future studies should identify the advantage and limitations of such tools, and how they can be used responsibly in academic research upholding the quality, contributions, impact and integrity of the work we publish.

Although AI systems are improving, a clear feature of present AI generated text is a certain blandness. It has few grammatical imperfections (although ChatGPT has clearly not read Fowler's Modern English Usage), but at the same time, it does not carry that whiff of life experience that infuses human writing styles. It certainly may be capable of summarizing a debate but does not yet seem capable of generating novel or counter-intuitive insights, or indeed of exercising moral judgements. In this vein, the meaning of ‘responsible HRM research’ should be debated and defined to evolve and adapt research methods and philosophies, in the age of machines becoming ghost writers.

A serious concern is AI's potential to generate fake quantitative or qualitative research results; understandably, there are likely to be much higher expectations for data transparency in the future, and indeed, greater documentation of the research process. AI detection software is rapidly advancing. This raises the question as to how journals might deploy this software in the future. In other words, what might be acceptable in helping automate the more mechanistic aspects of the research process, and when does falsification and crookery start? Again, this does hold a potentially career threatening risk for early academic adopters of AI. Any sharp practices that escape detection and enable publication now are likely to be exposed by more sophisticated AI detection software in the future; as with plagiarists, such individuals will be stalked by the risk of lustration for the remainder of their academic careers. Moreover, scholarly associations are currently devising guidelines around the usage of AI; such rules will inevitably lead to more restrictions than the present Wild West. Potential authors need to be very aware of this in the same manner as inhabitants of nineteenth century Tombstone, Arizona, might, with the wisdom of retrospect, have been well advised to take note of the prospective arrival in town of a group of lawmen (and, indeed, a certain dentist) more seriously.

It also does raise issues around academic careers; ambitious scholars typically advance their careers by constantly honing their critical thinking, analytical and writing skills. It could well be argued that such academic skills may become devalued. However, this does not take account of the worth of novel ideas that are founded in the human world, and indeed, the value of scholarly conversation. HRM is about people, and for all humanity's flaws, it is a general principle that members of a group have a better idea of their interests than outsiders, be the latter human or mechanical. There is also the Hawking principle to consider: ‘when more advanced life forms meet more primitive ones, it never ends well for the latter’.

# 7. CONCLUDING REMARKS

As with any new technology, AI may have open ended effects on HRM, the study of the same, and indeed, economy and society at large. Although it has clear potential to be beneficial, its full consequences are unknown. Indeed, the jury is out if genuine AI exists at the present time at all; however, even sophisticated sets of data compilation and problem-solving algorithms may pose great opportunities and risks. At HRMJ, we are seeking to promote greater understanding and debate around AI and its effects on the study and practice of HRM; the different thematic presentations in this HRMJ perspectives editorial do not provide a single set of definitive solutions or judgements, but rather a starting point on this journey of discovery.

# AFFILIATIONS

|1|Aston University, Birmingham, UK|
|---|---|
|2|Information, Operations and Management Sciences Department, TBS Business School, Toulouse, France|
|3|DAN Department of Management & Organizational Studies, Western University, London, Ontario, Canada|
|4|Cranfield University, Bedford, UK|
|5|University of Bath, Claver Down, UK|

# 46

# BUDHWAR et al.

6Trinity Business School, Trinity College Dublin, Dublin, Ireland

7School of Business, The George Washington University, Washington, District of Columbia, USA

8Monash Data Futures Institute, Monash University, Melbourne, Australia

9Department of Human Resources & Organizational Behavior, School of Business, Rutgers University-Camden, Camden, New Jersey, USA

10Human Resource Studies, Tilburg University, Tilburg, The Netherlands

11Utrecht University School of Governance, Utrecht University, Utrecht, The Netherlands

12Monash Business School, Monash University, Melbourne, Australia

13Department of Strategy & International Business, Birmingham Business School, University of Birmingham, Birmingham, UK

14Tulane University, A.B. Freeman School of Business, New Orleans, Los Angeles, USA

15Operations and Information Management Department, Aston Business School, College of Business and Social Science, Aston University, Birmingham, UK

16King’s Business School, King’s College London, London, UK

17Department of Organizational Science, University of North Carolina at Charlotte, Charlotte, North Carolina, USA

18College of Human and Social Futures, Newcastle Business School, The University of Newcastle (UON), Ourimbah, NSW, Australia

19Newcastle University Business School, Newcastle Upon Tyne, UK

20Henley Business School, University of Reading, Reading, UK

21NEOMA Business School, Reims, France

22Queen’s Business School, Queen’s University Belfast, Belfast, UK

23Department of Management, Birmingham Business School, University of Birmingham, Birmingham, UK

24Simon Fraser University, Burnaby, British Columbia, Canada

25Quinlan School of Business, Loyola University, Chicago, Illinois, USA

# ACKNOWLEDGMENTS

Several people helpfully advised on earlier versions of parts of this perspectives editorial including Dale M Clarke and Dickson Lukose. Fang Lee Cooke would like to acknowledge the support of the Economic and Social Research Council (ESRC) through the Digital Futures at Work Research Center (grant no. ES/S012532/1). No funding has been specifically sought by authors for the topic covered in this article.

# CONFLICT OF INTEREST STATEMENT

None of the authors have a conflict of interest to disclose.

# DATA AVAILABILITY STATEMENT

This is not an empirical article and does not use any form of primary data for analysis. Literature referred to in the article is listed in the bibliography.

# ORCID

Pawan Budhwar https://orcid.org/0000-0001-8915-6172

Soumyadeb Chowdhury https://orcid.org/0000-0002-8074-248X

Herman Aguinis https://orcid.org/0000-0002-3485-9484

Greg J. Bamber https://orcid.org/0000-0001-6646-3065

Jose R. Beltran https://orcid.org/0000-0002-3886-8142

Fang Lee Cooke https://orcid.org/0000-0003-0337-6591

Stephanie Decker https://orcid.org/0000-0003-0547-9594

David Guest https://orcid.org/0000-0003-0457-6077

Ashish Malik https://orcid.org/0000-0002-2422-4194

Vijay Pereira https://orcid.org/0000-0001-6755-0793

# BUDHWAR et al.

Shuang Ren https://orcid.org/0000-0002-8768-8447

Mark N. K. Saunders https://orcid.org/0000-0001-5176-8317

Arup Varma https://orcid.org/0000-0002-6530-5564

# ENDNOTES

1. With the exception of a paper in the proceedings of a conference by Schoder et al. (2019) we haven't found any reference to an Open Resource Based View in SHRM.
2. European legislation on General Data Protection Regulation.

# REFERENCES

1. Agrawal, A., Gans, J., & Goldfarb, A. (2022). ChatGPT and how AI disrupts industries. Harvard Business Review. https://hbr.org/2022/12/chatgpt-and-how-ai-disrupts-industries
2. Aguinis, H., Banks, G. C., Rogelberg, S. G., & Cascio, W. F. (2020). Actionable recommendations for narrowing the science-practice gap in open science. Organizational Behavior and Human Decision Processes, 158, 27–35. https://doi.org/10.1016/j.obhdp.2020.02.007
3. Aguinis, H., Ramani, R. S., & Alabduljader, N. (2018). What you see is what you get? Enhancing methodological transparency in management research. Academy of management annals. The Academy of Management Annals, 12(1), 83–110. https://doi.org/10.5465/annals.2016.0011
4. Aguinis, H., & Solarino, A. M. (2019). Transparency and replicability in qualitative research: The case of interviews with elite informants. Strategic Management Journal, 40(8), 1291–1315. https://doi.org/10.1002/smj.3015
5. AI Research Assistant Elicit. (2023). Elicit uses language models to help you automate research workflows, like parts of literature review. Retrieved from https://elicit.org/. Accessed on 12 May 2023.
6. Appel, G., Neelbauer, J., & Schweidel, D. A. (2023). Generative AI has an intellectual property problem. Harvard Business Review. https://hbr.org/2023/04/generative-ai-has-an-intellectual-property-problem
7. Aust, I., Matthews, B., & Muller-Camen, M. (2020). Common good HRM: A paradigm shift in sustainable HRM? Human Resource Management Review, 30(3), 100705. https://doi.org/10.1016/j.hrmr.2019.100705
8. Bamber, G. J., Cooke, F. L., Doellgast, V., & Wright, C. F. (2021). International and comparative employment relations: Global crises and institutional responses (7th ed.). SAGE.
9. Bandura, A. (1999). Moral disengagement in the perpetration of inhumanities. Personality and social psychology review. Personality and Social Psychology Review, 3(3), 193–209. https://doi.org/10.1207/s15327957pspr0303
10. Barney, J. (1991). Firm resources and sustained competitive advantage. Journal of Management, 17(1), 99–120. https://doi.org/10.1177/014920639101700108
11. Bednar, P. M., & Welch, C. (2020). Socio-technical perspectives on smart working: Creating meaningful and sustainable systems. Information Systems Frontiers, 22(2), 281–298. https://doi.org/10.1007/s10796-019-09921-1
12. Beer, M., Spector, B. A., Lawrence, P. R., Mills, D. Q., & Walton, R. E. (1984). Managing human assets: The groundbreaking harvard business school program. Free Press.
13. Bell, E., Dacin, M. T., & Toraldo, M. L. (2021). Craft imaginaries–past, present and future. Organization theory, 2(1), 263178772199114. https://doi.org/10.1177/2631787721991141
14. Bell, E., Mangia, G., Taylor, S., & Toraldo, M. L. (Eds.). (2018). The organization of craft work: Identities, meanings, and materiality. Routledge.
15. Bender, E. M., Gebru, T., McMillan-Major, A., & Shmitchell, S. (2021). On the dangers of stochastic parrots: Can language models be too big? FAccT 2021-proceedings of the 2021 ACM conference on fairness, accountability, and transparency, 610–623. https://doi.org/10.1145/3442188.3445922
16. Bidle, S. (2022). The internet’s new favourite AI proposes torturing Iranians and surveilling mosques. The Intercept Voices. Retrieved from https://theintercept.com/2022/12/08/openai-chatgpt-ai-bias-ethics/. Accessed on 11 May 2023.
17. Bing, M. (2023). Introducing the new bing. Your AI-powered copilot for the web. Retrieved from https://tinyurl.com/4vzmnrx7. Accessed on 11 May 2023.
18. Blau, F. D., & Kahn, L. M. (2007). The gender pay gap: Have women gone as far as they can? Academy of management perspectives. Academy of Management Perspectives, 21(1), 7–23. https://doi.org/10.5465/amp.2007.24286161
19. Blog, M. (2023). Microsoft 365 copilot—Your copilot for work. Web Link https://tinyurl.com/3b9dfykc. Accessed on 11 May 2023.
20. Bolina, J. (2023). How to defend against the rise of ChatGPT? Think like a poet. Washington Post. Retrieved from https://www.washingtonpost.com/opinions/2023/04/20/chatgpt-poetry-ai-language/. Accessed on 12 May 2023.
21. Boon, C., Eckardt, R., Lepak, D. P., & Boselie, P. (2018). Integrating strategic human capital and strategic human resource management. International Journal of Human Resource Management, 29(1), 34–67. https://doi.org/10.1080/09585192.2017.1380063

# References

Borden, J. (2014). MOOCs are dead - long live the MOOc. WIRED magazine, 2014. Retrieved from https://www.wired.com/insights/2014/08/moocs-are-dead-long-live-the-mooc/. Accessed on 12 May 2023.

Boston Consulting Group Generative AI. (2023). Retrieved from https://www.bcg.com/x/artificial-intelligence/generative-ai. Accessed on 12 May 2023.

Bosworth, B. and Collins, S. M., 2008. Accounting for growth: Comparing China and India. Journal of economic perspectives, 22(1), 45–66. https://doi.org/10.1257/jep.22.1.45

Brandtzaeg, P. B., & Følstad, A. (2017). Why people use chatbots. In Internet science: 4th international conference, INSCI 2017 (pp. 377–392). Springer International Publishing. https://doi.org/10.1007/978-3-319-70284-1

Breque, M., De Nul, L., & Petridis, A. (2021). Industry 5.0. Towards sustainable human-centric resilient European industry. European Director-General for Research and Innovation. Web Link Retrieved from https://tinyurl.com/4psv72b2. Accessed on 12 May 2023.

Broussard, M. (2018). Artificial unintelligence: How computers misunderstand the world. MIT Press.

Brynjolfsson, E. (1993). The productivity paradox of information technology. Communications of the ACM, 36(12), 66–77. https://doi.org/10.1145/163298.163309

Brynjolfsson, E., Rock, D., & Syverson, C. (2020). The productivity J-curve: How intangible complement general purpose technologies. National Bureau of Economic Research. Working paper 25148, Retrieved from http://www.nber.org/papers/w25148. Accessed on 12 May 2023.

Bubeck, S., Chandrasekaran, V., Eldan, R., Gehrke, J., Horvitz, E., Kamar, E., Lee, P., Lee, Y. T., Li, Y., Lundberg, S. and Nori, H., 2023. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712. https://doi.org/10.48550/arXiv.2303.12712

Budd, J. W., Pohler, D., & Huang, W. (2022). Making sense of (mis) matched frames of reference: A dynamic cognitive theory of (in) stability in HR practices. Industrial Relations: A Journal of Economy and Society, 61(3), 268–289. https://doi.org/10.1111/irel.12275

Budhwar, P., Malik, A., De Silva, M. T., & Thevisuthan, P. (2022). Artificial intelligence–challenges and opportunities for international HRM: A review and research agenda. International Journal of Human Resource Management, 33(6), 1065–1097. https://doi.org/10.1080/09585192.2022.2035161

Burger, B., Kanbach, D. K., Kraus, S., Breier, M., & Corvello, V. (2023). On the use of AI-based tools like ChatGPT to support management research. European Journal of Innovation Management, 26(7), 233–241. https://doi.org/10.1108/EJIM-02-2023-0156

ChatGPT (2023). Interview Participants’ ChatGPT Mar. 2023 version. Web Link https://chat.openai.com/. Accessed on 2023.

Chaudhary, M. (2023). How ChatGPT can Be a game changer in human resource management. Future of work. Spiceworks. February 21st. Web link. https://tinyurl.com/mwf63772. Accessed on 12 May 2023.

Cheng, X., Zhang, X., Cohen, J., & Mou, J. (2022). Human vs. AI: Understanding the impact of anthropomorphism on consumer response to chatbots from the perspective of trust and relationship norms. Information Processing and Management, 59(3), 102940. https://doi.org/10.1016/j.ipm.2022.102940

Chowdhury, S., Dey, P., Joel-Edgar, S., Bhattacharya, S., Rodriguez-Espindola, O., Abadie, A., & Truong, L. (2023). Unlocking the value of artificial intelligence in human resource management through AI capability framework. Human Resource Management Review, 33(1), 100899. https://doi.org/10.1016/j.hrmr.2022.100899

Chowdhury, S., Joel-Edgar, S., Dey, P. K., Bhattacharya, S., & Kharlamov, A. (2022). Embedding transparency in artificial intelligence machine learning models: Managerial implications on predicting and explaining employee turnover. International Journal of Human Resource Management, 1–32. https://doi.org/10.1080/09585192.2022.2066981

Clayton, J. (2023). Sam altman: CEO of OpenAI calls for US to regulate artificial intelligence. BBC news. 16 may 2023. Web link. https://www.bbc.com/news/world-us-canada-65616866. Accessed on 17 May 2023.

Clayton UTZ. (2023). Generative AI miniseries - opportunities and risks for Australian organisations, in Ep2: The workplace and employment implications of generative AI – Risky business? Web link. Retrieved from https://tinyurl.com/5n6tx94y. Accessed on 12 May 2023.

Cortada, J. W. (2006). The digital hand: How information technology changed the way industries worked in the United States. Business History Review, 80(4), 755–766. https://doi.org/10.2307/25097268

Cortada, J. W. (2013). How new technologies spread: Lessons from computing technologies. Technology and Culture, 54(2), 229–261. https://doi.org/10.1353/tech.2013.0081. https://www.jstor.org/stable/24468014

Cox, J. (2023). AI anxiety: The workers who fear losing their jobs to artificial intelligence. BBC Worklife. Retrieved from https://tinyurl.com/k6j4swez. Accessed on 16 May 2023.

Crafts, N. (2010). The contribution of new technology to economic growth: Lessons from economic history. Revista de Historia Económica-Journal of Iberian and Latin American Economic History, 28(3), 409–440. https://doi.org/10.1017/S0212610910000157

Dasborough, M. T. (2023). Awe-inspiring advancements in AI: The impact of ChatGPT on the field of organizational behavior. Journal of Organizational Behavior, 44(2), 177–179. https://doi.org/10.1002/job.2695

# BUDHWAR et al.

Data Scientist. (2023). Internet searches will change forever with Google’s “Magi”. Retrieved from https://tinyurl.com/3d-j94ryf. Accessed on 11 May 2023.

Daugherty, P. R., Wilson, H. J., & Chowdhury, R. (2018). Using artificial intelligence to promote diversity. MIT Sloan Management Review. Magazine Winter 2019 Issue/Frontiers/Research Highlight. Retrieved from https://sloanreview.mit.edu/article/using-artificial-intelligence-to-promote-diversity/. Accessed on 12 May 2023.

Daugherty, P. R., Wilson, H. J., & Michelman, P. (2019). Revisiting the jobs artificial intelligence will create. MIT Sloan Management Review. https://mitsmr.com/2QZT4mE

Davenport (2023). How Morgan Stanley is training GPT to help financial advisors. Forbes. Retrieved from https://tinyurl.com/32zsapxz. Accessed on 12 May 2023.

Davis, F. D. (1989). Perceived usefulness, perceived ease of use, and user acceptance of information technology. MIS Quarterly, 13(3), 319–339. https://doi.org/10.2307/249008

De Vos, A., Van der Heijden, B. I., & Akkermans, J. (2020). Sustainable careers: Towards a conceptual model. Journal of Vocational Behavior, 117, 103196. https://doi.org/10.1016/j.jvb.2018.06.011

Decker, S., Nix, A., Kirsch, D., & Venkata, S. K. (2022). The dotcom archive: Contextualizing email archives. Web Link https://dotcomarchive.bristol.ac.uk/. Accessed on 12 May 2023.

Deloitte (2017). The 2017 Deloitte state of cognitive survey. Web Link https://tinyurl.com/4kn2c35s. Accessed on 08 May 2023.

DeNisi, A., Murphy, K., Varma, A., & Budhwar, P. (2021). Performance management systems and multinational enterprises: Where we are and where we should go. Human Resource Management, 60(5), 707–713. https://doi.org/10.1002/hrm.22080

DeNisi, A., Varma, A., & Budhwar, P. S. (2023). Performance management around the globe: Where are we now? In A. Varma, P. Budhwar, & A. DeNisi (Eds.), Performance management systems: A global perspective (2nd ed.). Routledge.

DeNisi, A. S., & Murphy, K. R. (2017). Performance appraisal and performance management: 100 years of progress? Journal of Applied Psychology, 102(3), 421–433. https://doi.org/10.1037/apl0000085

Derico, B., & Kleinman, Z. (2023). OpenAI announces ChatGPT successor GPT-4. BBC News Online. Web Link https://www.bbc.com/news/technology-64959346. Accessed on 12 May 2023.

Dey, P. K., Chowdhury, S., Abadie, A., Vann Yaroson, E., & Sarkar, S. (2023). Artificial intelligence-driven supply chain resilience in Vietnamese manufacturing small-and medium-sized enterprises. International Journal of Production Research, 1–40. https://doi.org/10.1080/00207543.2023.2179859

Dey, P. K., Malesios, C., Chowdhury, S., Saha, K., Budhwar, P., & De, D. (2022). Adoption of circular economy practices in small and medium-sized enterprises: Evidence from Europe. International Journal of Production Economics, 248, 108496. https://doi.org/10.1016/j.ijpe.2022.108496

Dilmegani, C. (2023). Generative AI ethics: Top 6 concerns. Research AIMultiple. Web link. https://research.aimultiple.com/generative-ai-ethics/. Accessed on 12 May 2023.

Dwivedi, Y. K., Kshetri, N., Hughes, L., Slade, E. L., Jeyaraj, A., Kar, A. K., Wright, R., Koohang, A., Raghavan, V., Ahuja, M., Albanna, H., Albashrawi, M. A., Al-Busaidi, A. S., Balakrishnan, J., Barlette, Y., Basu, S., Bose, I., Brooks, L., & Buhalis, D. (2023). “So what if ChatGPT wrote it?” Multidisciplinary perspectives on opportunities, challenges and implications of generative conversational AI for research, practice and policy. International Journal of Information Management, 71, 102642. https://doi.org/10.1016/j.ijinfomgt.2023.102642

Eddleston, K., Hughes, M., & Deeds, D. (2023). Family business.org's editorial guidelines for the use of generative AI tools. Family Business.org. Web Link. https://tinyurl.com/45e3f95y. Accessed on 12 May 2023.

Edlich, A., Ip, F., & Whiteman, R. (2018). How bots, algorithms, and artificial intelligence are reshaping the future of corporate support functions. McKinsey Digital, McKinsey and Company. Retrieved from https://tinyurl.com/5fdtpzmj. Accessed on 12 May 2023.

Edwards, M. R., Charlwood, A., Guenole, N., & Marler, J. (2022). HR analytics: An emerging field finding its place in the world alongside simmering ethical challenges. Human Resource Management Journal. https://doi.org/10.1111/1748-8583.12435

Eisenhardt, K. M. (1989). Building theories from case study research. Academy of Management Review, 14(4), 532–550. https://doi.org/10.5465/amr.1989.4308385

Elsevier. (2023). The use of AI and AI-assisted technologies in scientific writing. Retrieved from: https://tinyurl.com/5dwsyntf. Accessed on May-12.

Farndale, E., Bonache, J., McDonnell, A., & Kwon, B. (2023). Positioning context front and center in international human resource management research. Human Resource Management Journal, 33(1), 1–16. https://doi.org/10.1111/1748-8583.12483.

FDA. (2022). Do cell phones pose a health hazard? US food and drug administration website. Retrieved from https://tinyurl.com/ycxyax5p. Accessed on 12 May 2023.

Fitzpatrick, K. K., Darcy, A., & Vierhile, M. (2017). Delivering cognitive behavior therapy to young adults with symptoms of depression and anxiety using a fully automated conversational agent (Woebot): A randomized controlled trial. JMIR mental health, 4(2), e7785. https://doi.org/10.2196/mental.7785

# References

BUDHWAR et al.

Forscey, D., Bateman, J., Beecroft, N., & Woods, B. (2022). Systemic cyber risk: A primer. Carnegie Endowment for International Peace.

Francis, J. J., Johnston, M., Robertson, C., Glidewell, L., Entwistle, V., Eccles, M. P., & Grimshaw, J. M. (2010). What is an adequate sample size? Operationalising data saturation for theory-based interview studies. Psychology and Health, 25(10), 1229–1245. https://doi.org/10.1080/08870440903194015

Gendron, Y., Andrew, J., & Cooper, C. (2022). The perils of artificial intelligence in academic publishing. Critical Perspectives on Accounting, 87, 102411. https://doi.org/10.1016/j.cpa.2021.102411

Gilpin, L. H., Bau, D., Yuan, B. Z., Bajwa, A., Specter, M., & Kagal, L. (2018). October. Explaining explanations: An overview of interpretability of machine learning. In 2018 IEEE 5th International Conference on data science and advanced analytics (DSAA) (pp. 80–89). IEEE. https://doi.org/10.1109/DSAA.2018.00018

Glaser, B. G., & Strauss, A. L. (1967). The discovery of grounded theory.

Goldfarb, B., & Kirsch, D. A. (2019). Bubbles and crashes: The boom and bust of technological innovation. Stanford University Press.

Graeber, D. (2018). Bullshit jobs: A theory. New York: Simon and Schuster.

Grant, R. M. (1996). Toward a knowledge-based theory of the firm. Strategic Management Journal, 17(S2), 109–122. https://doi.org/10.1002/smj.4250171110

Griffi, A. (2023). ChatGPT creators try to use artificial intelligence to explain itself – And come across major problems. INDEPENDENT. Web link. https://tinyurl.com/bdfzbvux. Accessed on 12 May 2023.

Guardian News. (2023). Godfather of AI’ Geoffrey Hinton quits Google and warns over dangers of misinformation. Web Link Retrieved from https://tinyurl.com/3se42t5e. Accessed on 11 May 2023.

Guest, D., Knox, A., & Warhurst, C. (2022). Humanizing work in the digital age: Lessons from socio-technical systems and quality of working life initiatives. Human Relations, 75(8), 1461–1482. https://doi.org/10.1177/00187267221092674

Guest, G., Bunce, A., & Johnson, L. (2006). How many interviews are enough? An experiment with data saturation and variability. Field Methods, 18(1), 59–82. https://journals.sagepub.com/doi/10.1177/1525822X05279903

Habermas, J. (1984). The theory of communicative action. In Reason and rationalization of society (Vol. 1). Heinemann.

Habermas, J. (1992). Moral consciousness and communicative action. Polity Press.

Hatzius, J., Briggs, J., Kodnani, D., & Pierdomenico, G. (2023). The potentially large effects of artificial intelligence on economic growth (Briggs/Kodnani) goldman Sachs economic reports, 2023. Retrieved from https://tinyurl.com/4n-hmyc58. Accessed on 12 May 2023.

Hennink, M. M., Kaiser, B. N., & Marconi, V. C. (2017). Code saturation versus meaning saturation: How many interviews are enough? Qualitative Health Research, 27(4), 591–608. https://doi.org/10.1177/1049732316665344

Howlett, E. (2023). Third of HR professionals want to use ChatGPT at work, exclusive data reveals. People Management. Web Link https://tinyurl.com/yu6j8fkf

Hyman, L. (2023). It’s not the end of work. It’s the end of boring work. Retrieved from https://tinyurl.com/3tbap77h. Accessed on 12 April 2023.

Ioakimidis, V., & Maglajlic, R. A. (2023). Neither ‘neo-luddism’ nor ‘neo-positivism’; rethinking social work’s positioning in the context of rapid technological change. British Journal of Social Work, 53(2), 693–697. https://doi.org/10.1093/bjsw/bcad081

Keegan, A., & Den Hartog, D. (2019). Doing it for themselves? Performance appraisal in project-based organisations, the role of employees, and challenges to theory. Human Resource Management Journal, 29(2), 217–237. https://doi.org/10.1111/1748-8583.12216

Kelan, E. K. (2023). Algorithmic inclusion: Shaping the predictive algorithms of artificial intelligence in hiring. Human Resource Management Journal. https://doi.org/10.1111/1748-8583.12511

Kelly, J. (2023). Goldman Sachs predicts 300 million jobs will be lost or degraded by artificial intelligence. Forbes. Web Link https://tinyurl.com/3xb437rb. Accessed on 04 May 2023.

Kim, T., Lee, H., Kim, M. Y., Kim, S., & Duhachek, A. (2022). AI increases unethical consumer behavior due to reduced anticipatory guilt. Journal of the Academy of Marketing Science, 1–17. https://doi.org/10.1007/s11747-021-00832-9

Kiron, D. (2022). AI can change how you measure- and how you manage. MIT Sloan Management Review. Spring 2022 Issue. Retrieved from https://sloanreview.mit.edu/article/ai-can-change-how-you-measure-and-how-you-manage/. Accessed on 12 May 2023.

Korzynski, P., Mazurek, G., Altmann, A., Ejdys, J., Kazlauskaite, R., Paliszkiewicz, J., Wach, K., & Ziemba, E. (2023). Generative artificial intelligence as a new context for management theories: Analysis of ChatGPT. Central European Management Journal, 31(1), 3–13. https://doi.org/10.1108/CEMJ-02-2023-0091

Lamertz, K., Foster, W. M., Coraiola, D. M., & Kroezen, J. (2016). New identities from remnants of the past: An examination of the history of beer brewing in Ontario and the recent emergence of craft breweries. Business History, 58(5), 796–828. https://doi.org/10.1080/00076791.2015.1065819

Lance, C. E. (2011). More statistical and methodological myths and urban legends. Organizational Research Methods, 14(2), 279–286. https://doi.org/10.1177/1094428110391814

# BUDHWAR et al.

51

# References

Lecher, C. (2019). How Amazon automatically tracks and fires warehouse workers for ‘productivity’ the Verge 25 April, Web Link. https://tinyurl.com/khpf227w. Accessed on 06 May 2023.

Lips, H. M. (2013). The gender pay gap: Challenging the rationalizations. Perceived equity, discrimination, and the limits of human capital models. Sex Roles, 68(3–4), 169–185. https://doi.org/10.1007/s11199-012-0165-z

Loynds, J. (2023). How to jailbreak ChatGPT: Best prompts and more. Dexerto website. Retrieved from https://www.dexerto.com/tech/how-to-jailbreak-chatgpt-2143442/. Accessed on 14 May 2023.

Luthra, S., Kumar, A., Zavadskas, E. K., Mangla, S. K., & Garza-Reyes, J. A. (2020). Industry 4.0 as an enabler of sustainability diffusion in supply chain: An analysis of influential strength of drivers in an emerging economy. International Journal of Production Research, 58(5), 1505–1521. https://doi.org/10.1080/00207543.2019.1660828

Maersk Press Release. (2022). A.P. Moller - Maersk and IBM to discontinue TradeLens, a blockchain-enabled global trade platform. Maersk website. Web Link. Retrieved from https://tinyurl.com/2mdsvtvv. Accessed on 10 May 2023.

Malik, A., Budhwar, P., & Kazmi, B. A. (2022). Artificial intelligence (AI)-assisted HRM: Towards an extended strategic framework. Human Resource Management Review, 33(1), 100940. https://doi.org/10.1016/j.hrmr.2022.100940

Malik, A., Budhwar, P., Patel, C., & Srikanth, N. R. (2022). May the bots be with you! Delivering HR cost-effectiveness and individualized employee experiences in an MNE. International Journal of Human Resource Management, 33(6), 1148–1178. https://doi.org/10.1080/09585192.2020.1859582

Margherita, A. (2022). Human resources analytics: A systematization of research topics and directions for future research. Human Resource Management Review, 32(2), 100795. https://doi.org/10.1016/j.hrmr.2020.100795

Marian, M. (2023). AI could cause a mass-extinction of language – and ways of thinking. Washington Post. Retrieved from https://tinyurl.com/mryjkesv. Accessed on 10 May 2023.

Maslej, N., Fattorini, L., Brynjolfsson, E., Etchemendy, J., Ligett, K., Lyons, T., Manyika, J., Ngo, H., Niebles, J. C., Parli, V., Shoham, Y., Wald, R., Clark, J., & Perrault, R. (2023). The AI index 2023 annual report. AI index steering committee, Institute for human-centered AI. Stanford University. Retrieved from https://tinyurl.com/yckv22zr. Accessed on 10 May 2023.

McKinsey and Company Featured Insights. (2023). What is generative AI. Retrieved from https://www.mckinsey.com/featured-insights/mckinsey-explainers/what-is-generative-ai. Accessed on 12 May 2023.

McKinsey Global Institute. (2017). A future that works: Automation, employment, and productivity. McKinsey and Company. Retrieved from https://tinyurl.com/mprjz7ne. Accessed on 12 May 2023.

McNeill, J. (2011). An environmental history of the industrial revolution.” environment and society portal. August 2011. Retrieved from https://tinyurl.com/7233u3a2. Accessed on 11 May 2023.

Meta AI Research. (2023). Introducing LLaMA: A foundational, 65-billion-parameter large language model. Web Link Retrieved from https://ai.facebook.com/blog/large-language-model-llama-meta-ai/. Accessed on 11 May 2023.

Meta Help Center. (2023). How does Facebook use artificial intelligence to moderate content? Facebook website. Web link. Retrieved from https://www.facebook.com/help/1584908458516247. Accessed on 12 May 2023.

Milanez, A. (2023). The impact of AI on the workplace: Evidence from OECD case studies of AI implementation. OECD social, No. 289, OECD Publishing. Employment and Migration Working Papers. https://doi.org/10.1787/2247ce58-en

Milne-Smith, A. (2016). Shattered minds: Madmen on the railways, 1860–80. Journal of Victorian Culture, 21(1), 21–39. https://doi.org/10.1080/13555502.2015.1118851

Mingers, J. (2000). What is it to be critical? Teaching a critical approach to management undergraduates. Management Learning, 31(2), 219–237. https://doi.org/10.1177/1350507600312005

Morse, J. M. (1995). The significance of saturation. Qualitative Health Research, 5(2), 147–149. https://doi.org/10.1177/104973239500500201

Mthuli, S. A., Ruffin, F., & Singh, N. (2022). Define, Explain, Justify, Apply’(DEJA): An analytic tool for guiding qualitative research sample size. International Journal of Social Research Methodology, 25(6), 809–821. https://doi.org/10.1080/13645579.2021.1941646

Mukherjee, D., Lim, W. M., Kumar, S., & Donthu, N. (2022). Guidelines for advancing theory and practice through bibliometric research. Journal of Business Research, 148, 101–115. https://doi.org/10.1016/j.jbusres.2022.04.042

Musser, M., Gelles, R., Kinoshita, R., Aiken, C., & Lohn, A. (2023). The main resource is the human. Georgetown University’s Center for Security and Emerging Technology. Retrieved from https://tinyurl.com/4xru7afw. Accessed on 30 April 2023.

Myklebust, J. P. (2023). ‘Universities adjust to ChatGPT, but the real AI lies ahead’. University World News. 4 March [online]. Web Link Retrieved from https://tinyurl.com/2p8524fe. Accessed on 25 April 2023.

Nature (2023). Submission guidelines’ nature [online]. Retrieved from https://www.nature.com/nbt/submission-guidelines/preparing-your-submission. Accessed on 17 April 2023.

Newman, D. T., Fast, N. J., & Harmon, D. J. (2020). When eliminating bias isn’t fair: Algorithmic reductionism and procedural justice in human resource decisions. Organizational Behavior and Human Decision Processes, 160, 149–167. https://doi.org/10.1016/j.obhdp.2020.03.008

Newsroom, S. (2023). Stripe and OpenAI collaborate to monetize OpenAI’s flagship products and enhance Stripe with GPT-4. Web Link https://stripe.com/en-gb-fr/newsroom/news/stripe-and-openai. Accessed on 11 May 2023.

# BUDHWAR et al.

Noble, S. U. (2018). Algorithms of oppression: How search engines reinforce racism. NYU Press.

OpenAI, G. P. T. 4 research blog. 2023. GPT-4. Web link. https://openai.com/research/gpt-4. Accessed on 12 May 2023.

OpenAI Blog. (2022). Introducing ChatGPT. Web link. Retrieved from https://openai.com/blog/chatgpt#OpenAI. Accessed on 10 May 2023.

OpenAI Customer Stories. (2023). How Iceland is using GPT-4 to preserve its language. Retrieved from https://openai.com/customer-stories/government-of-iceland. Accessed on 11 May 2023.

OpenAI Research Blog LM. (2023). Aligning language models to follow instructions. Web Link. Retrieved from https://openai.com/research/instruction-following. Accessed on 12 May 2023.

Ortiz, S. (2023). What is ChatGPT and why does it matter? Here's what you need to know. ZDNET. Web link. https://tinyurl.com/328sznjz. Accessed on 10 May 2023.

Ouyang, L., Wu, J., Jiang, Xu, Almeida, D., Leike, K., & Lowe, R. (2023). Training language models to follow instructions with human feedback. OpenAI White Paper Report. Web Link. https://cdn.openai.com/papers/Training_language_models_to_follow_instructions_with_human_feedback.pdf. Accessed on 12 May 2023.

Paauwe, J. (2004). HRM and performance: Achieving long-term viability. Oxford University Press.

Paauwe, J., & Boselie, P. (2003). Challenging ‘strategic HRM’ and the relevance of the institutional setting. Human Resource Management Journal, 13(3), 56–70. https://doi.org/10.1111/j.1748-8583.2003.tb00098.x

Pan, Y., Froese, F., Liu, N., Hu, Y., & Ye, M. (2022). The adoption of artificial intelligence in employee recruitment: The influence of contextual factors. International Journal of Human Resource Management, 33(6), 1125–1147. https://doi.org/10.1080/09585192.2021.1879206

Parisi, K. (2023). This company is experimenting with using ChatGPT for performance reviews. HR Brew. Web Link. https://tinyurl.com/tmuyau88. Accessed on 11 May 2023.

Pereira, V., Hadjielias, E., Christofi, M., & Vrontis, D. (2023). A systematic literature review on the impact of artificial intelligence on workplace outcomes: A multi-process perspective. Human Resource Management Review, 33(1), 100857. https://doi.org/10.1016/j.hrmr.2021.100857

Perrigo, B. (2023). Exclusive: OpenAI used Kenyan workers on less than $2 per hour to make ChatGPT less toxic. Time (January 18). Web link. https://tinyurl.com/37v295ra. Accessed on 23 May 2023.

Pfeffer, J. (1994). Competitive advantage through people. Harvard University Press.

Pomeranz, K. (2009). The great divergence: China, Europe, and the making of the modern world economy. Princeton University Press.

Posthuma, R. A., Charles Campion, M., & Campion, M. A. (2018). A taxonomic foundation for evidence-based research on employee performance management. European Journal of Work & Organizational Psychology, 27(2), 168–187. https://doi.org/10.1080/1359432X.2018.1438411

Prakash, P. (2023). Alphabet CEO Sundar Pichai said the Bard chatbot would make mistakes. It called Google a monopoly and asked for the government to break it up. Fortune. Web Link. Retrieved from https://tinyurl.com/y9j65cye. Accessed on 10 May 2023.

Prikshat, V., Patel, P., Varma, A., & Ishizaka, A. (2022). A multi-stakeholder ethical framework for AI-augmented HRM. International Journal of Manpower, 43(1), 226–250. https://doi.org/10.1108/IJM-03-2021-0118

Purcell, J., & Hutchinson, S. (2007). Front-line managers as agents in the HRM-performance causal chain: Theory, analysis and evidence. Human Resource Management Journal, 17(1), 3–20. https://doi.org/10.1111/j.1748-8583.2007.00022.x

PWC. (2016). 3D printing in US industrial manufacturing. PriceWaterhouseCoopers. Website. Web Link. Retrieved from https://tinyurl.com/27bzjmks. Accessed on 10 May 2023.

PWC. (2022). PwC’s global workforce hopes and fears survey 2022. Web Link. Retrieved from https://tinyurl.com/32tdbx8z. Accessed on 15 May 2023.

Quiroz-Gutierrez, M. (2023). Disney joins Microsoft and Snapchat as latest big-name company to slink away from metaverse ambitions. Fortune. Web Link. https://tinyurl.com/2p9ha9es. Accessed on 12 May 2023.

Ren, S., Cooke, F. L., Stahl, G. K., Fan, D., & Timming, A. R. (2023). Advancing the sustainability agenda through strategic human resource management: Insights and suggestions for future research. Human Resource Management, 62(3), 251–265. https://doi.org/10.1002/hrm.22169

Ren, S., & Jackson, S. (2020). HRM institutional entrepreneurship for sustainable business organizations. Human Resource Management Review, 30(3), 100691. https://doi.org/10.1016/j.hrmr.2019.100691

Renkema, M. (2023). ChatGPT is booming! En dat is ook een nieuwe uitdaging voor HR-professionals en CHRO's. Chief Human Resource Officer. Web Link. https://tinyurl.com/25z2tu52. Accessed on 05 May 2023.

Renkema, M., Drost, J., & Bondarouk, T. (2022). A conceptual framework of knowledge workers’ experiences in collaborating with artificial intelligence: Implications for work design. In In 12th biennial international conference of the Dutch HRM network 2022: HRM for resilient societies: A call for actionable knowledge.

Reuters (2023). EU proposes new copyright rules for generative AI. Web Link. https://tinyurl.com/2s4x3z2f. Accessed on 12 May 2023.

Sachs, G. (2023). Artificial intelligence. Web link. https://tinyurl.com/28yvhjr5. Accessed on April 16, 2023.

# BUDHWAR et al.

# References

Saul, J., & Bass, D. (2023). Artificial intelligence is booming—So is its carbon footprint. Bloomberg News. Web Link https://tinyurl.com/eruhxdhv. Accessed on 10 May 2023.

Saunders, M. N., & Townsend, K. (2016). Reporting and justifying the number of interview participants in organization and workplace research. British Journal of Management, 27(4), 836–852. https://doi.org/10.1111/1467-8551.12182

Schoder, D., Schlagwein, D., & Fischbach, K. (2019). Open resource-based view (orbv): A theory of resource openness. In ICIS 2019 proceedings. 9. Germany. Retrieved from https://tinyurl.com/2s3etfm5

Schrage, M., Kiron, D., Candelon, F., Khodabandeh, S., & Chu, M. (2023). AI is helping companies redefine, not just improve performance. MIT Sloan Management Review. Big Idea: AI and Business Strategy/Research Highlight Issue March. Web Link https://sloanreview.mit.edu/article/ai-is-helping-companies-redefine-not-just-improve-performance/. Accessed on 12 May 2023.

Schulman, J., Wolski, F., Dhariwal, P., Radford, A. and Klimov, O., 2017. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347.

Schwartz, R., Vassilev, A., Greene, K., Perine, L., & Burt, A. (2022). Towards a standard for identifying bias in Artificial Intelligence. Department of Commerce’s National Institute of Standards and Technology. https://doi.org/10.6028/NIST.SP.1270

Schweizer, K. (2022). Artificial unintelligence: How computers misunderstand the world. The European Legacy, 27(7–8), 7–8. https://doi.org/10.1080/10848770.2022.2110366

Shet, S. V., & Pereira, V. (2021). Proposed managerial competencies for Industry 4.0–Implications for social sustainability. Technological Forecasting and Social Change, 173, 121080. https://doi.org/10.1016/j.techfore.2021.121080

Spataro, J. (2023). Introducing Microsoft 365 copilot – your copilot for work. Official Microsoft blog, 2023. Web Link https://tinyurl.com/3s8xjmcn. Accessed on 10 May 2023.

Stahl, G. K., Brewster, C. J., Collings, D. G., & Hajro, A. (2020). Enhancing the role of human resource management in corporate sustainability and social responsibility: A multi-stakeholder, multidimensional approach to HRM. Human Resource Management Review, 30(3), 100708. https://doi.org/10.1016/j.hrmr.2019.100708

Suddaby, R., Ganzin, M., & Minkus, A. (2017). Craft, magic and the Re-enchantment of the world. European Management Journal, 35(3), 285–296. https://doi.org/10.1016/j.emj.2017.03.009

Suseno, Y., Chang, C., Hudik, M., & Fang, E. S. (2022). Beliefs, anxiety and change readiness for artificial intelligence adoption among human resource managers: The moderating role of high-performance work systems. International Journal of Human Resource Management, 33(6), 1209–1236. https://doi.org/10.1080/09585192.2021.1931408

Susskind, R. E., & Susskind, D. (2022). The future of the professions: How technology will transform the work of human experts, updated edition. Oxford University Press.

Taylor, A. (2020). Smartphone pinky’ and other injuries caused by excessive phone use. The conversation, 2020. Web Link https://tinyurl.com/6keemvsk. Accessed on 11 May 2023.

Tcharnetsky, M., & Vogt, F. (2023). The OSQE model: The ai cycle against the shortage of skilled professionals: A holistic solution approach based on artificial intelligence in times of demographic change. https://doi.org/10.20944/preprints202302.0069.v

Teicher, J., Van Gramberg, B., & Bamber, G. J. (2023). Understanding workplace conflict and its management in the context of COVID-19. In A. Avgar, D. Hann, R. Lamare, & D. Nash (Eds.), The evolution of workplace dispute resolution: International perspectives. Labor and Employment Relations Association.

The Economist. (2020). Businesses are finding AI hard to adopt. Web Link. Retrieved from https://tinyurl.com/2p8ne738. Accessed on 12 May 2023.

The Royal Society. (2019). Explainable AI: The basics - POLICY BRIEFING. Web Link Retrieved from https://tinyurl.com/wkkevmu9. Accessed on 10 May 2023.

Thompson, E. P. (1966). The making of the English working class. Victor Gollancz Ltd.

Thorbecke, C. (2023). Google shares lose $100 billion after company’s AI chatbot makes an error during demo. CNN Business. Web Link https://edition.cnn.com/2023/02/08/tech/google-ai-bard-demo-error/index.html. Accessed on 10 May 2023.

Trist, E., Higgin, G., Murray, H., & Pollock, A. (1963). Organizational choice. Tavistock Publications.

Uszkoreit, J. (2017). Transformer: A novel neural network architecture for language understanding. Google Research Blog. Web Link https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html. Accessed on 12 May 2023.

Vallance, C. (2023). AI could replace equivalent of 300 million jobs – report. BBC News Technology, 28. Web Link https://www.bbc.com/news/technology-65102150. Accessed on 05 May 2023.

van Dis, E. A. M., Bollen, J., Zuidema, W., van Rooij, R., & Bockting, C. (2023). ChatGPT: Five priorities for research. Nature, 614(7947), 224–226.

Varma, A., Dawkins, C., & Chaudhuri, K. (2022). Artificial intelligence and people management: A critical assessment through the ethical lens. Human Resource Management Review, 33(1), 100923. https://doi.org/10.1016/j.hrmr.2022.100923

Varma, A., Jaiswal, A., Pereira, V., & Kumar, Y. L. N. (2022). Leader-member exchange in the age of remote work. Human Resource Development International, 25(2), 219–230. https://doi.org/10.1080/13678868.2022.2047873

# References

BUDHWAR et al.

Venkata, S. K., Decker, S., Kirsch, D. A., & Nix, A. (2021). EMCODIST: A context-based search tool for email archives. In 2021 IEEE international conference on big data (big data) (pp. 2281–2290). IEEE. https://doi.org/10.1109/BigData52589.2021.9671832

Vincent, J. (2023). Meta open-sources multisensory AI model that combines six types of data. Verge. Web Link https://tinyurl.com/2s3jucwd. Accessed on 12 May 2023.

von Krogh, G., Roberson, Q., & Gruber, M. (2023). Recognizing and utilizing novel research opportunities with artificial intelligence. Academy of Management Journal, 66(2), 367–373. https://doi.org/10.5465/amj.2023.4002

Wang, Z., Cai, S. A., Ren, S., & Singh, S. K. (2023). Green operational performance in a high-tech industry: Role of green HRM and green knowledge. Journal of Business Research, 160, 113761. https://doi.org/10.1016/j.jbusres.2023.113761

Westerman, J. W., Rao, M. B., Vanka, S., & Gupta, M. (2020). Sustainable human resource management and the triple bottom line: Multi-stakeholder strategies, concepts, and engagement. Human Resource Management Review, 30(3), 100742. https://doi.org/10.1016/j.hrmr.2020.100742

Willig, C., & Rogers, W. S. (2017). The SAGE handbook of qualitative research in Psychology (2nd ed.). Sage.

Wilson, H. J., Daugherty, P., & Bianzino, N. (2017). The jobs that artificial intelligence will create. MIT Sloan Management Review, 58(4), 14. http://mitsmr.com/2odREFJ

Wolf, Z. B. (2023). AI can be racist, sexist and creepy. What should we do about it? CNN what matters. Web Link Retrieved from https://tinyurl.com/yc4u46d3. Accessed on 12 May 2023.

Xue, M., Cao, X., Feng, X., Gu, B., & Zhang, Y. (2022). Is college education less necessary with AI? Evidence from firm-level labor structure changes. Journal of Management Information Systems, 39(3), 865–905. https://doi.org/10.1080/07421222.2022.2096542

Yadav, R., Chaudhary, N. S., Kumar, D. and Saini, D., 2022. Mediating and moderating variables of employee relations and sustainable organizations: A systematic literature review and future research agenda. International journal of organizational analysis, (ahead-of-print). https://doi.org/10.1108/IJOA-12-2021-3091

Yakar, T. (2023). GPT-4 and Salesforce potential features. ApexHours website. Web Link https://www.apexhours.com/gpt-4-salesforce-potential-features/. Accessed on 11 May 2023.

Zu, D. (2023). Collective action and AI: The next stage in accelerating digital transformation. University of Bremen. Retrieved from https://tinyurl.com/yckywptz. Accessed on 25 April 2023.

# How to cite this article

Budhwar, P., Chowdhury, S., Wood, G., Aguinis, H., Bamber, G. J., Beltran, J. R., Boselie, P., Lee Cooke, F., Decker, S., DeNisi, A., Dey, P. K., Guest, D., Knoblich, A. J., Malik, A., Paauwe, J., Papagiannidis, S., Patel, C., Pereira, V., Ren, S., Rogelberg, S., Saunders, M. N. K., Tung, R. L., … Varma, A. (2023). Human resource management in the age of generative artificial intelligence: Perspectives and research directions on ChatGPT. Human Resource Management Journal, 1–54. https://doi.org/10.1111/1748-8583.12524